#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Tennis Match Prediction Model Training Script - Version 3

This script trains an XGBoost model to predict tennis match outcomes
using features generated by generate_features_v3.py.

The script follows best practices for XGBoost model training:
- Strict time-based train/val/test split to prevent data leakage
- Hyperparameter tuning with Optuna
- Proper handling of categorical features
- Feature selection based on importance
- Comprehensive model evaluation
- Model and pipeline saving for inference
"""

import os
import sys
import json
import time
import pickle
import logging
import warnings
import argparse
from pathlib import Path
from datetime import datetime
from typing import Optional, Any, Dict, List, Tuple, Union, cast

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import xgboost as xgb
import optuna
import psycopg2
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, roc_curve, auc,
    precision_recall_curve, log_loss, brier_score_loss,
    confusion_matrix, balanced_accuracy_score
)
from sklearn.calibration import calibration_curve
from psycopg2 import pool
from dotenv import load_dotenv
import psutil

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("train_model_v3.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("tennis_predictor")

# Project directories
PROJECT_DIR = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_DIR / "data"
OUTPUT_DIR = PROJECT_DIR / "output" / "v3"
MODELS_DIR = OUTPUT_DIR / "models"
PLOTS_DIR = OUTPUT_DIR / "plots"

# Input/output file paths
INPUT_FILE = DATA_DIR / "processed" / "tennis_matches_features_v3.csv"
MODEL_OUTPUT = MODELS_DIR / "tennis_model_v3.json"
PIPELINE_OUTPUT = MODELS_DIR / "tennis_pipeline_v3.pkl"
METRICS_OUTPUT = OUTPUT_DIR / "model_metrics_v3.json"
HYPERPARAMS_OUTPUT = OUTPUT_DIR / "hyperparameters_v3.json"
OPTUNA_STUDY_OUTPUT = OUTPUT_DIR / "optuna_study_v3.pkl"

# Training configuration
TRAIN_SPLIT = 0.7
VAL_SPLIT = 0.15
TEST_SPLIT = 0.15
RANDOM_SEED = 42
TARGET_COLUMN = 'player1_win'
ID_COLUMNS = ['player1_id', 'player2_id', 'tournament_id', 'match_id']
DATETIME_COLUMNS = ['tournament_date']
CATEGORICAL_COLUMNS_PATTERN = [
    'surface', 'tournament_level', 'round', 'court_type', 
    'tournament_name', 'player1_hand', 'player2_hand'
]

# Tennis surfaces for surface-specific evaluation
SURFACES = ["Hard", "Clay", "Grass", "Carpet"]

# Database configuration
DB_BATCH_SIZE = 10000  # Number of records to fetch in each database batch
DB_TIMEOUT_SECONDS = 30  # Database query timeout in seconds

# Default CPU threads to use if GPU is not available
DEFAULT_CPU_THREADS = max(4, os.cpu_count() - 2) if os.cpu_count() else 4

def detect_optimal_device() -> dict:
    """
    Detect the optimal device for XGBoost training.
    Tries GPU first with CUDA, then falls back to CPU with multithreading.
    
    Returns:
        Dict with optimal parameters for tree_method and nthread
    """
    # Try GPU first
    try:
        # Create small test matrix to verify GPU works
        test_matrix = xgb.DMatrix(np.array([[1, 2], [3, 4]]), label=np.array([0, 1]))
        test_params = {'tree_method': 'gpu_hist'}
        xgb.train(test_params, test_matrix, num_boost_round=1)
        
        logger.info("CUDA GPU acceleration available and will be used")
        return {
            'tree_method': 'gpu_hist',
            'gpu_id': 0
        }
    except Exception as e:
        logger.info(f"GPU acceleration not available ({str(e)}), falling back to CPU with multithreading")
        
        # Determine optimal number of threads for CPU
        cpu_threads = DEFAULT_CPU_THREADS
        logger.info(f"Using {cpu_threads} CPU threads for XGBoost training")
        
        return {
            'tree_method': 'hist',
            'nthread': cpu_threads
        }

# XGBoost baseline parameters with optimal device detection
device_params = detect_optimal_device()
BASELINE_PARAMS = {
    'objective': 'binary:logistic',
    'eval_metric': ['logloss', 'error', 'auc'],
    'grow_policy': 'lossguide',  # Can improve accuracy
    'max_depth': 6,
    'min_child_weight': 1,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': RANDOM_SEED,
    'verbosity': 0
}
# Add device-specific parameters
BASELINE_PARAMS.update(device_params)

def get_database_connection() -> psycopg2.extensions.connection:
    """
    Create a database connection using environment variables.
    
    Returns:
        Database connection
    """
    # Load environment variables
    load_dotenv()
    
    # Get database URL from environment
    database_url = os.environ.get("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL not found in environment variables")
    
    try:
        # Convert postgres:// to postgresql:// if needed (psycopg2 requirement)
        if database_url.startswith('postgres://'):
            database_url = database_url.replace('postgres://', 'postgresql://', 1)
            
        # Add connection timeout and statement timeout settings
        connection = psycopg2.connect(
            database_url,
            connect_timeout=10,
            options=f"-c statement_timeout={DB_TIMEOUT_SECONDS * 1000}"
        )
        logger.info("Successfully connected to database")
        return connection
    except Exception as e:
        logger.error(f"Error connecting to database: {e}")
        raise

def load_data_from_database(limit: Optional[int] = None, 
                           progress_tracker: Optional['ProgressTracker'] = None) -> pd.DataFrame:
    """
    Load the tennis match features from the database efficiently.
    Handles the match_features table created by generate_features_v3.py.
    
    Args:
        limit: Optional limit on number of rows to fetch
        progress_tracker: Optional progress tracker
        
    Returns:
        DataFrame with tennis match features
    """
    logger.info("Loading data from database (match_features table)...")
    
    # Connect to database
    conn = get_database_connection()
    
    try:
        # Define the query with dynamic row limit
        limit_clause = f"LIMIT {limit}" if limit else ""
        
        # Use batched loading to handle large datasets efficiently
        offset = 0
        dataframes = []
        total_rows = 0
        
        if limit:
            total_to_fetch = limit
        else:
            # Get total count first
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM match_features")
                total_to_fetch = cursor.fetchone()[0]
        
        logger.info(f"Fetching up to {total_to_fetch} rows from database")
        pbar = tqdm(total=total_to_fetch, desc="Loading data from database")
        
        while True:
            # Define batch query
            query = f"""
            SELECT *
            FROM match_features
            ORDER BY tournament_date ASC
            {limit_clause}
            OFFSET {offset}
            LIMIT {DB_BATCH_SIZE}
            """
            
            # Load batch
            batch_df = pd.read_sql(query, conn)
            
            # If batch is empty, we're done
            if len(batch_df) == 0:
                break
                
            # Append to list of dataframes
            dataframes.append(batch_df)
            
            # Update counts
            rows_fetched = len(batch_df)
            total_rows += rows_fetched
            pbar.update(rows_fetched)
            
            # Check if we've reached the limit
            if limit and total_rows >= limit:
                break
                
            # Update offset for next batch
            offset += DB_BATCH_SIZE
        
        pbar.close()
        
        # Combine all batches
        if dataframes:
            df = pd.concat(dataframes, ignore_index=True)
            
            # Convert date columns to datetime
            if 'tournament_date' in df.columns:
                df['tournament_date'] = pd.to_datetime(df['tournament_date'])
            
            # Handle column name mappings to maintain compatibility with the code
            column_mapping = {
                'player1_id': 'player1_id',
                'player2_id': 'player2_id'
            }
            
            # Rename columns if needed
            df = df.rename(columns=column_mapping)
            
            # Ensure result is an integer (1 for win, 0 for loss)
            if 'result' in df.columns:
                df['result'] = df['result'].astype(int)
            
            # Sort by date
            df = df.sort_values(by='tournament_date').reset_index(drop=True)
            
            logger.info(f"Loaded {len(df)} match features")
            logger.info(f"Feature columns: {len(df.columns)} columns including date, IDs, etc.")
            
            # Log information about key features
            numerical_cols = [col for col in df.columns if df[col].dtype.kind in 'fib' 
                             and col not in ['match_id', 'player1_id', 'player2_id', 'result']]
            logger.info(f"Number of numerical features: {len(numerical_cols)}")
            
            # Log sample values
            if len(df) > 0:
                logger.info(f"Time span: {df['tournament_date'].min()} to {df['tournament_date'].max()}")
                
            if progress_tracker:
                progress_tracker.update("Data loading complete")
            
            return df
        else:
            logger.warning("No data retrieved from database")
            return pd.DataFrame()
    
    finally:
        conn.close()

# Note: Only database loading is used in this workflow.
# File-based loading has been removed as it was not being used.

class ProgressTracker:
    """Helper class to track and log progress during model training."""
    
    def __init__(self, total_steps: int, task_description: str):
        """
        Initialize the progress tracker.
        
        Args:
            total_steps: Total number of steps in the process
            task_description: Description of the task
        """
        self.total_steps = total_steps
        self.current_step = 0
        self.task_description = task_description
        self.start_time = time.time()
        self.last_update_time = self.start_time
        
        logger.info(f"Starting {task_description} with {total_steps} steps")
    
    def update(self, step_description: str) -> None:
        """
        Update the progress tracker.
        
        Args:
            step_description: Description of the current step
        """
        self.current_step += 1
        current_time = time.time()
        elapsed = current_time - self.last_update_time
        total_elapsed = current_time - self.start_time
        
        logger.info(f"[{self.current_step}/{self.total_steps}] {step_description} "
                   f"(step: {elapsed:.2f}s, total: {total_elapsed:.2f}s)")
        
        self.last_update_time = current_time
    
    def get_progress(self) -> float:
        """
        Get the current progress as a fraction.
        
        Returns:
            Progress as a float between 0 and 1
        """
        return self.current_step / self.total_steps


class XGBoostProgressCallback(xgb.callback.TrainingCallback):
    """
    Callback to track XGBoost training progress.
    """
    def __init__(self, total_rounds: int):
        self.total_rounds = total_rounds
        self.current_round = 0
        self.start_time = time.time()
        
    def after_iteration(self, model, epoch, evals_log):
        """Log progress after each iteration."""
        self.current_round += 1
        if self.current_round % 10 == 0 or self.current_round == self.total_rounds:
            elapsed = time.time() - self.start_time
            progress = (self.current_round / self.total_rounds) * 100
            
            if self.current_round < self.total_rounds:
                est_remaining = (elapsed / self.current_round) * (self.total_rounds - self.current_round)
                logger.info(f"XGBoost training: {progress:.1f}% complete. Est. remaining time: {est_remaining:.1f}s")
            else:
                logger.info(f"XGBoost training: 100% complete. Total time: {elapsed:.1f}s")
                
        return False  # Continue training


def create_time_based_train_val_test_split(
    df: pd.DataFrame,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    progress_tracker: Optional[ProgressTracker] = None
) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Create a strict time-based split of the data to prevent data leakage.
    
    Args:
        df: DataFrame containing the tennis match data with features
        train_ratio: Proportion of data to use for training
        val_ratio: Proportion of data to use for validation
        test_ratio: Proportion of data to use for testing
        progress_tracker: Optional progress tracker
        
    Returns:
        Tuple of (train_df, val_df, test_df)
    """
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-10:
        raise ValueError(f"Split ratios must sum to 1.0, got {train_ratio + val_ratio + test_ratio}")
    
    # Sort by date to ensure chronological ordering
    df_sorted = df.sort_values('tournament_date')
    
    # Calculate split indices
    n = len(df_sorted)
    train_end = int(n * train_ratio)
    val_end = train_end + int(n * val_ratio)
    
    # Create splits
    train_df = df_sorted.iloc[:train_end].copy()
    val_df = df_sorted.iloc[train_end:val_end].copy()
    test_df = df_sorted.iloc[val_end:].copy()
    
    # Verify time-based integrity of splits
    train_max_date = train_df['tournament_date'].max()
    val_min_date = val_df['tournament_date'].min()
    val_max_date = val_df['tournament_date'].max()
    test_min_date = test_df['tournament_date'].min()
    
    if val_min_date < train_max_date or test_min_date < val_max_date:
        logger.warning("Time-based split integrity issue detected!")
        logger.warning(f"Training max date: {train_max_date.date()}")
        logger.warning(f"Validation min date: {val_min_date.date()}")
        logger.warning(f"Validation max date: {val_max_date.date()}")
        logger.warning(f"Test min date: {test_min_date.date()}")
    
    # Log the split information
    logger.info(f"Training data: {len(train_df)} samples from {train_df['tournament_date'].min().date()} to {train_df['tournament_date'].max().date()}")
    logger.info(f"Validation data: {len(val_df)} samples from {val_df['tournament_date'].min().date()} to {val_df['tournament_date'].max().date()}")
    logger.info(f"Test data: {len(test_df)} samples from {test_df['tournament_date'].min().date()} to {test_df['tournament_date'].max().date()}")
    
    if progress_tracker:
        progress_tracker.update("Time-based split created")
    
    return train_df, val_df, test_df


def get_feature_columns(df: pd.DataFrame, progress_tracker: Optional[ProgressTracker] = None) -> list[str]:
    """
    Identify feature columns for training.
    
    Args:
        df: DataFrame containing tennis match data with features
        progress_tracker: Optional progress tracker
        
    Returns:
        List of feature column names
    """
    logger.info("Identifying feature columns...")
    
    # Exclude non-feature columns
    exclude_columns = [
        'match_id', 'tournament_date', 'tournament_id', 'player1_id', 'player2_id', 
        'player1_name', 'player2_name', 'player1_win'
    ]
    
    # Get all column names 
    all_columns = df.columns.tolist()
    
    # Filter out non-feature columns
    feature_cols = [col for col in all_columns if col not in exclude_columns]
    
    # Log feature types
    elo_features = [col for col in feature_cols if 'elo' in col.lower()]
    win_rate_features = [col for col in feature_cols if 'win_rate' in col.lower()]
    streak_features = [col for col in feature_cols if 'streak' in col.lower()]
    serve_features = [col for col in feature_cols if any(term in col.lower() for term in 
                                                      ['serve', 'ace', 'bp_saved'])]
    return_features = [col for col in feature_cols if any(term in col.lower() for term in 
                                                       ['return', 'bp_conversion'])]
    surface_features = [col for col in feature_cols if any(surface in col.lower() for surface in 
                                                       ['hard', 'clay', 'grass', 'carpet'])]
    
    # Log feature counts by category
    logger.info(f"Total features: {len(feature_cols)}")
    logger.info(f"Elo features: {len(elo_features)}")
    logger.info(f"Win rate features: {len(win_rate_features)}")
    logger.info(f"Streak features: {len(streak_features)}")
    logger.info(f"Serve features: {len(serve_features)}")
    logger.info(f"Return features: {len(return_features)}")
    logger.info(f"Surface-specific features: {len(surface_features)}")
    
    # Identify categorical features
    categorical_features = []
    
    # Check for categorical features in DataFrame
    for col in feature_cols:
        if df[col].dtype == 'object' or df[col].dtype.name == 'category':
            categorical_features.append(col)
    
    # Display categorical features
    if categorical_features:
        logger.info(f"Categorical features: {categorical_features}")
    
    if progress_tracker:
        progress_tracker.update("Feature columns identified")
    
    return feature_cols


def prepare_features(
    train_df: pd.DataFrame, 
    val_df: pd.DataFrame, 
    test_df: pd.DataFrame, 
    feature_cols: List[str],
    progress_tracker: Optional[ProgressTracker] = None
) -> Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray], List[int]]:
    """
    Prepare feature matrices and labels for training, validation, and testing.
    Following XGBoost best practices:
    - Let XGBoost handle missing values by keeping NaNs
    - Identify categorical features but leave them as is
    - No scaling needed for tree-based models
    
    Args:
        train_df: Training data
        val_df: Validation data
        test_df: Test data
        feature_cols: List of feature columns
        progress_tracker: Optional progress tracker
        
    Returns:
        Tuple of:
        - (X_train, X_val, X_test): Feature matrices
        - (y_train, y_val, y_test): Target vectors
        - categorical_feature_indices: Indices of categorical features
    """
    logger.info("Preparing features following XGBoost best practices")
    
    # Check for potential data leakage by ensuring:
    # 1. All features are from past matches only (should be handled by generate_features_v3.py)
    # 2. No future information is used in features
    
    # Verify time-based split to confirm no leakage
    train_end_date = train_df['tournament_date'].max()
    val_start_date = val_df['tournament_date'].min()
    val_end_date = val_df['tournament_date'].max()
    test_start_date = test_df['tournament_date'].min()
    
    if not (train_end_date < val_start_date and val_end_date < test_start_date):
        logger.warning(f"Potential data leakage in time-based split: train ends {train_end_date}, "
                      f"val starts {val_start_date}, val ends {val_end_date}, test starts {test_start_date}")
        logger.warning("This may be due to multiple matches on the same day.")
    
    # Identify categorical features
    categorical_features = []
    categorical_indices = []
    
    for i, col in enumerate(feature_cols):
        # Check if column is categorical
        if col in train_df.columns and (
            train_df[col].dtype.name == 'category' or 
            (train_df[col].dtype == 'object' and train_df[col].nunique() < 100)
        ):
            categorical_features.append(col)
            categorical_indices.append(i)
    
    if categorical_features:
        logger.info(f"Found {len(categorical_features)} categorical features: {categorical_features}")
    
    # Extract features and labels
    X_train = train_df[feature_cols].values
    y_train = train_df['result'].values
    
    X_val = val_df[feature_cols].values
    y_val = val_df['result'].values
    
    X_test = test_df[feature_cols].values
    y_test = test_df['result'].values
    
    # Check for features with excessive missing values (>50%)
    # Generate_features_v3.py already handles most missing values, but we check anyway
    missing_pct = (np.isnan(X_train).sum(axis=0) / X_train.shape[0]) * 100
    high_missing_features = [(feature_cols[i], pct) for i, pct in enumerate(missing_pct) if pct > 50]
    
    if high_missing_features:
        logger.warning(f"Features with >50% missing values (potential quality issues):")
        for feature, pct in high_missing_features:
            logger.warning(f"  - {feature}: {pct:.1f}% missing")
        logger.info("XGBoost will handle missing values, but features with too many missing values may be uninformative")
    
    # Log feature statistics
    logger.info(f"Training set shape: {X_train.shape}, class distribution: {np.bincount(y_train)}")
    logger.info(f"Validation set shape: {X_val.shape}, class distribution: {np.bincount(y_val)}")
    logger.info(f"Test set shape: {X_test.shape}, class distribution: {np.bincount(y_test)}")
    
    # Log missing value statistics
    train_missing_rates = (np.isnan(X_train).sum(axis=0) / X_train.shape[0]) * 100
    high_missing_features = [(feature_cols[i], rate) for i, rate in enumerate(train_missing_rates) if rate > 10]
    
    if high_missing_features:
        logger.info("Features with high missing rates (>10%):")
        for feature, rate in high_missing_features[:10]:  # Show only top 10 to avoid log flood
            logger.info(f"  - {feature}: {rate:.2f}%")
        if len(high_missing_features) > 10:
            logger.info(f"  - ... and {len(high_missing_features) - 10} more")
        logger.info("XGBoost will handle these missing values optimally")
    
    if progress_tracker:
        progress_tracker.update("Feature preparation complete")
    
    return (X_train, X_val, X_test), (y_train, y_val, y_test), categorical_indices


def tune_hyperparameters(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    feature_names: List[str],
    categorical_indices: List[int] = None,
    progress_tracker: Optional[ProgressTracker] = None,
    n_trials: int = 50,
    timeout: int = 3600
) -> Dict[str, Any]:
    """
    Tune XGBoost hyperparameters using Optuna.
    
    Args:
        X_train: Training features
        y_train: Training labels
        X_val: Validation features
        y_val: Validation labels
        feature_names: List of feature names
        categorical_indices: Indices of categorical features (if any)
        progress_tracker: Optional progress tracker
        n_trials: Number of trials for optimization
        timeout: Timeout in seconds
        
    Returns:
        Dictionary of optimized hyperparameters
    """
    logger.info(f"Tuning hyperparameters with Optuna (n_trials={n_trials}, timeout={timeout}s)")
    
    # Get optimal device parameters once before tuning
    device_params = detect_optimal_device()
    
    # Define objective function for Optuna
    def objective(trial: optuna.Trial) -> float:
        """Objective function for hyperparameter optimization."""
        # Define hyperparameter search space based on XGBoost best practices
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            
            # Learning rate (smaller values require more trees but generalize better)
            'eta': trial.suggest_float('eta', 0.01, 0.2, log=True),
            
            # Tree complexity parameters
            'max_depth': trial.suggest_int('max_depth', 3, 9),
            'max_leaves': trial.suggest_int('max_leaves', 32, 512),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'gamma': trial.suggest_float('gamma', 0, 0.5),
            
            # Subsampling parameters to prevent overfitting
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            
            # Regularization parameters (more focused ranges for tennis prediction)
            'lambda': trial.suggest_float('lambda', 1.0, 10.0),  # L2 regularization
            'alpha': trial.suggest_float('alpha', 0.01, 1.0),    # L1 regularization
            
            # Class imbalance parameter
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.8, 1.2),
            
            # Performance optimizations for large datasets
            'grow_policy': 'lossguide',  # Grow trees by loss reduction
            'max_bin': trial.suggest_int('max_bin', 128, 512),  # Number of bins for histogram
            
            # Categorical feature handling
            'max_cat_to_onehot': trial.suggest_int('max_cat_to_onehot', 4, 16),
            
            # Let XGBoost handle missing values optimally
            'missing': float('nan')
        }
        
        # Add device-specific parameters from the pre-tuning detection
        params.update(device_params)
        
        # Number of boosting rounds
        num_boost_round = trial.suggest_int('num_boost_round', 500, 3000)
        
        # Enable categorical feature support if needed
        if categorical_indices and len(categorical_indices) > 0:
            params['enable_categorical'] = True
        
        # Create DMatrix objects - memory efficient data structure
        dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names,
                             enable_categorical=params.get('enable_categorical', False))
        dval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_names,
                           enable_categorical=params.get('enable_categorical', False))
        
        # Set up categorical features if present
        if categorical_indices:
            for cat_idx in categorical_indices:
                dtrain.set_feature_types(['c' if i == cat_idx else 'q' for i in range(X_train.shape[1])])
                dval.set_feature_types(['c' if i == cat_idx else 'q' for i in range(X_val.shape[1])])
        
        # Train model with early stopping
        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, "validation-logloss")
        evals = [(dtrain, "train"), (dval, "validation")]
        
        # Use num_boost_round from the trial
        bst = xgb.train(
            params, dtrain, num_boost_round=num_boost_round, 
            evals=evals, early_stopping_rounds=50, 
            callbacks=[pruning_callback], verbose_eval=False
        )
        
        # Return validation metric as objective value (lower is better for logloss)
        return float(bst.best_score)
    
    # Create Optuna study with pruning to eliminate unpromising trials early
    study_name = f"xgboost_tennis_prediction_{time.strftime('%Y%m%d_%H%M%S')}"
    study = optuna.create_study(
        direction="minimize",  # Minimize logloss
        study_name=study_name,
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
    )
    
    # Run hyperparameter optimization
    try:
        study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)
        
        # Get best parameters
        best_params = study.best_params
        
        # Extract num_boost_round from best params
        num_boost_round = best_params.pop('num_boost_round', 1000)
        
        # Get optimal device parameters again for final model
        final_device_params = detect_optimal_device()
        
        # Add fixed parameters
        best_params.update({
            'objective': 'binary:logistic',
            'eval_metric': ['logloss', 'auc'],  # Track both metrics
            'grow_policy': 'lossguide',
            'missing': float('nan'),
            'num_boost_round': num_boost_round
        })
        
        # Add device-specific parameters
        best_params.update(final_device_params)
        
        # Enable categorical support if needed
        if categorical_indices and len(categorical_indices) > 0:
            best_params['enable_categorical'] = True
        
        # Log best parameters
        logger.info(f"Best hyperparameters: {best_params}")
        logger.info(f"Best validation logloss: {study.best_value:.6f}")
        
        # Save study results
        with open(OPTUNA_STUDY_OUTPUT, 'wb') as f:
            pickle.dump(study, f)
        logger.info(f"Optuna study saved to {OPTUNA_STUDY_OUTPUT}")
        
        # Create optimization history plot
        try:
            fig = plot_optimization_history(study)
            fig.write_image(str(PLOTS_DIR / "optuna_history.png"))
            
            fig = plot_param_importances(study)
            fig.write_image(str(PLOTS_DIR / "optuna_param_importance.png"))
            
            logger.info("Optuna plots saved to plots directory")
        except Exception as e:
            logger.warning(f"Failed to create Optuna plots: {e}")
        
        if progress_tracker:
            progress_tracker.update(f"Hyperparameter tuning complete - best validation logloss: {study.best_value:.6f}")
        
        return best_params
    
    except Exception as e:
        logger.error(f"Error in hyperparameter tuning: {e}")
        logger.info("Falling back to default parameters")
        
        # Return default parameters on error
        default_params = MODEL_PARAMS.copy()
        
        # Enable categorical support if needed
        if categorical_indices and len(categorical_indices) > 0:
            default_params['enable_categorical'] = True
            
        if progress_tracker:
            progress_tracker.update("Using default parameters due to tuning error")
            
        return default_params


def train_model(
    X_train: np.ndarray, 
    y_train: np.ndarray, 
    X_val: np.ndarray, 
    y_val: np.ndarray, 
    feature_names: list[str],
    categorical_features: list[int] = None,
    params: dict[str, Any] = None,
    early_stopping_rounds: int = 50,
    progress_tracker: Optional[ProgressTracker] = None
) -> tuple[xgb.Booster, dict[str, list[float]]]:
    """
    Train an XGBoost model with early stopping.
    
    Args:
        X_train: Training feature matrix
        y_train: Training target vector
        X_val: Validation feature matrix
        y_val: Validation target vector
        feature_names: List of feature names
        categorical_features: List of indices of categorical features
        params: XGBoost parameters (uses BASELINE_PARAMS if None)
        early_stopping_rounds: Number of rounds for early stopping
        progress_tracker: Optional progress tracker
        
    Returns:
        Tuple of (trained model, evaluation results)
    """
    logger.info("Training XGBoost model with early stopping")
    
    start_time = time.time()
    
    if params is None:
        # Start with baseline params
        params = BASELINE_PARAMS.copy()
    else:
        # If params are provided but don't have device settings, add them
        if 'tree_method' not in params:
            device_params = detect_optimal_device()
            params.update(device_params)
    
    # Enable categorical feature support if categorical features are present
    if categorical_features and len(categorical_features) > 0:
        params['enable_categorical'] = True
        logger.info(f"Enabling categorical feature support for {len(categorical_features)} features")
    
    # Create DMatrix format for better memory efficiency and faster processing
    logger.info("Creating DMatrix for efficient memory usage")
    dtrain = xgb.DMatrix(
        X_train, 
        label=y_train, 
        feature_names=feature_names,
        enable_categorical=params.get('enable_categorical', False)
    )
    
    if categorical_features:
        feature_types = ['c' if i in categorical_features else 'q' for i in range(X_train.shape[1])]
        dtrain.set_feature_types(feature_types)
    
    dval = xgb.DMatrix(
        X_val, 
        label=y_val, 
        feature_names=feature_names,
        enable_categorical=params.get('enable_categorical', False)
    )
    
    if categorical_features:
        feature_types = ['c' if i in categorical_features else 'q' for i in range(X_val.shape[1])]
        dval.set_feature_types(feature_types)
    
    # Set up evaluation metrics
    watchlist = [(dtrain, 'train'), (dval, 'validation')]
    
    # Create callback for progress tracking
    callbacks = []
    if progress_tracker:
        progress_callback = XGBoostProgressCallback(params.get('num_boost_round', 1000))
        callbacks.append(progress_callback)
    
    # Train model with early stopping
    logger.info(f"Starting training with early stopping (patience: {early_stopping_rounds})")
    
    num_boost_round = params.pop('num_boost_round', 1000)
    
    # Log memory usage before training
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    logger.info(f"Memory usage before training: {memory_before:.2f} MB")
    
    # Train the model
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=num_boost_round,
        evals=watchlist,
        early_stopping_rounds=early_stopping_rounds,
        callbacks=callbacks,
        verbose_eval=100  # Log progress every 100 rounds
    )
    
    # Log memory usage after training
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    logger.info(f"Memory usage after training: {memory_after:.2f} MB (change: {memory_after - memory_before:.2f} MB)")
    
    # Get evaluation results
    evals_result = model.eval_set([(X_val, y_val)], iteration=model.best_iteration)
    
    # Log best iteration and score
    logger.info(f"Best iteration: {model.best_iteration}")
    logger.info(f"Best validation score: {model.best_score}")
    logger.info(f"Training completed in {time.time() - start_time:.2f} seconds")
    
    if progress_tracker:
        progress_tracker.update(f"Model training completed (best iteration: {model.best_iteration})")
    
    return model, model.evals_result()


def evaluate_model(
    model: xgb.Booster, 
    X: np.ndarray, 
    y: np.ndarray, 
    feature_names: List[str],
    dataset_name: str = "Test",
    threshold: float = 0.5,
    surfaces: List[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> Dict[str, Any]:
    """
    Evaluate model performance on a dataset with comprehensive metrics.
    
    Args:
        model: Trained XGBoost model
        X: Feature matrix
        y: Target vector
        feature_names: List of feature names
        dataset_name: Name of the dataset for reporting
        threshold: Classification threshold
        surfaces: List of surfaces to evaluate separately
        progress_tracker: Optional progress tracker
        
    Returns:
        Dictionary of evaluation metrics
    """
    if progress_tracker:
        progress_tracker.update(f"Evaluating model on {dataset_name} dataset")
    
    logger.info(f"Evaluating model on {dataset_name} dataset with {X.shape[0]} samples")
    
    # Create DMatrix for efficient prediction
    device_params = detect_optimal_device()
    dtest = xgb.DMatrix(X, label=y, feature_names=feature_names)
    
    # Make predictions
    y_pred_proba = model.predict(dtest)
    y_pred = (y_pred_proba > threshold).astype(int)
    
    # Calculate standard classification metrics
    accuracy = accuracy_score(y, y_pred)
    precision = precision_score(y, y_pred)
    recall = recall_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    
    # Calculate ROC AUC
    try:
        fpr, tpr, _ = roc_curve(y, y_pred_proba)
        roc_auc = auc(fpr, tpr)
    except Exception as e:
        logger.warning(f"Could not calculate ROC AUC: {e}")
        roc_auc = None
    
    # Calculate average precision (PR AUC)
    try:
        pr_auc = average_precision_score(y, y_pred_proba)
    except Exception as e:
        logger.warning(f"Could not calculate PR AUC: {e}")
        pr_auc = None
    
    # Calculate log loss
    try:
        logloss = log_loss(y, y_pred_proba)
    except Exception as e:
        logger.warning(f"Could not calculate log loss: {e}")
        logloss = None
    
    # Calculate Brier score (calibration metric)
    try:
        brier = brier_score_loss(y, y_pred_proba)
    except Exception as e:
        logger.warning(f"Could not calculate Brier score: {e}")
        brier = None
    
    # Confusion matrix for showing class distribution
    try:
        cm = confusion_matrix(y, y_pred)
        tn, fp, fn, tp = cm.ravel()
    except Exception:
        tn, fp, fn, tp = 0, 0, 0, 0
    
    # Get detailed classification report
    try:
        class_report = classification_report(y, y_pred, output_dict=True)
    except Exception as e:
        logger.warning(f"Could not generate classification report: {e}")
        class_report = {}
    
    # Log metrics
    logger.info(f"{dataset_name} metrics:")
    logger.info(f"  Accuracy: {accuracy:.4f}")
    logger.info(f"  Precision: {precision:.4f}")
    logger.info(f"  Recall: {recall:.4f}")
    logger.info(f"  F1 Score: {f1:.4f}")
    if roc_auc:
        logger.info(f"  ROC AUC: {roc_auc:.4f}")
    if pr_auc:
        logger.info(f"  PR AUC: {pr_auc:.4f}")
    if logloss:
        logger.info(f"  Log Loss: {logloss:.4f}")
    if brier:
        logger.info(f"  Brier Score: {brier:.4f}")
    
    # Tennis-specific metrics: evaluate on different surfaces
    surface_metrics = {}
    
    if surfaces is not None and hasattr(df, 'surface'):
        for surface in surfaces:
            surface_idx = df['surface'] == surface
            
            if sum(surface_idx) > 0:
                y_true_surface = y[surface_idx]
                y_pred_surface = y_pred[surface_idx]
                y_proba_surface = y_pred_proba[surface_idx]
                
                if len(y_true_surface) > 0:
                    accuracy_surface = accuracy_score(y_true_surface, y_pred_surface)
                    precision_surface = precision_score(y_true_surface, y_pred_surface, zero_division=0)
                    recall_surface = recall_score(y_true_surface, y_pred_surface, zero_division=0)
                    f1_surface = f1_score(y_true_surface, y_pred_surface, zero_division=0)
                    
                    # ROC AUC for this surface
                    try:
                        roc_auc_surface = auc(*roc_curve(y_true_surface, y_proba_surface)[:2])
                    except Exception:
                        roc_auc_surface = None
                    
                    surface_metrics[surface] = {
                        'accuracy': accuracy_surface,
                        'precision': precision_surface,
                        'recall': recall_surface,
                        'f1': f1_surface,
                        'roc_auc': roc_auc_surface,
                        'count': int(sum(surface_idx))
                    }
                    
                    logger.info(f"Metrics for {surface} surface (n={sum(surface_idx)}):")
                    logger.info(f"  Accuracy: {accuracy_surface:.4f}")
                    logger.info(f"  F1 Score: {f1_surface:.4f}")
                    if roc_auc_surface:
                        logger.info(f"  ROC AUC: {roc_auc_surface:.4f}")
            else:
                logger.info(f"No {dataset_name} samples for {surface} surface")
    
    # Calculate metrics by confidence range (measure of prediction certainty)
    confidence_bins = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    confidence_metrics = []
    
    for i in range(len(confidence_bins) - 1):
        lower = confidence_bins[i]
        upper = confidence_bins[i + 1]
        
        # Find predictions in this confidence range (from both sides of 0.5)
        mask = (
            ((y_pred_proba >= lower) & (y_pred_proba < upper)) | 
            ((y_pred_proba <= (1 - lower)) & (y_pred_proba > (1 - upper)))
        )
        
        if sum(mask) > 0:
            bin_acc = accuracy_score(y[mask], y_pred[mask])
            
            confidence_metrics.append({
                'confidence_range': f"{lower:.1f}-{upper:.1f}",
                'accuracy': bin_acc,
                'count': int(sum(mask)),
                'percentage': float(sum(mask) / len(y) * 100)
            })
            
            logger.info(f"Accuracy for confidence {lower:.1f}-{upper:.1f}: {bin_acc:.4f} " 
                      f"(n={sum(mask)}, {sum(mask) / len(y) * 100:.1f}%)")
    
    # Tennis-specific: Calculate metrics for underdogs and favorites
    # In tennis betting, favorites have higher probability of winning
    favorite_mask = y_pred_proba >= 0.6  # Probability threshold for favorites
    underdog_mask = y_pred_proba < 0.4   # Probability threshold for underdogs
    
    # Calculate metrics for favorites
    if sum(favorite_mask) > 0:
        favorite_acc = accuracy_score(y[favorite_mask], y_pred[favorite_mask])
        favorite_metrics = {
            'accuracy': favorite_acc,
            'count': int(sum(favorite_mask)),
            'percentage': float(sum(favorite_mask) / len(y) * 100)
        }
        logger.info(f"Favorite (p ≥ 0.6) accuracy: {favorite_acc:.4f} " 
                   f"(n={sum(favorite_mask)}, {sum(favorite_mask) / len(y) * 100:.1f}%)")
    else:
        favorite_metrics = {'accuracy': 0, 'count': 0, 'percentage': 0}
    
    # Calculate metrics for underdogs
    if sum(underdog_mask) > 0:
        underdog_acc = accuracy_score(y[underdog_mask], 1 - y_pred[underdog_mask])  # Inverse prediction for underdogs
        underdog_metrics = {
            'accuracy': underdog_acc,
            'count': int(sum(underdog_mask)),
            'percentage': float(sum(underdog_mask) / len(y) * 100)
        }
        logger.info(f"Underdog (p < 0.4) accuracy: {underdog_acc:.4f} " 
                   f"(n={sum(underdog_mask)}, {sum(underdog_mask) / len(y) * 100:.1f}%)")
    else:
        underdog_metrics = {'accuracy': 0, 'count': 0, 'percentage': 0}
    
    # Compile all metrics
    metrics = {
        'overall': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'log_loss': logloss,
            'brier_score': brier,
            'confusion_matrix': {
                'tn': int(tn if 'tn' in locals() else 0),
                'fp': int(fp if 'fp' in locals() else 0),
                'fn': int(fn if 'fn' in locals() else 0),
                'tp': int(tp if 'tp' in locals() else 0)
            },
            'count': len(y)
        },
        'class_report': class_report,
        'by_surface': surface_metrics,
        'by_confidence': confidence_metrics,
        'favorites': favorite_metrics,
        'underdogs': underdog_metrics,
        'threshold': threshold,
        'dataset': dataset_name
    }
    
    if progress_tracker:
        progress_tracker.update(f"Model evaluation on {dataset_name} data complete")
    
    return metrics


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, 
                          output_path: Optional[Union[str, Path]] = None,
                          title: str = "Confusion Matrix",
                          progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save confusion matrix.
    
    Args:
        y_true: True labels
        y_pred: Predicted labels
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    cm = confusion_matrix(y_true, y_pred)
    
    # Calculate percentages
    cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Plot counts
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax1)
    ax1.set_xlabel('Predicted')
    ax1.set_ylabel('Actual')
    ax1.set_title(f'{title} (Counts)')
    ax1.set_xticklabels(['Loss', 'Win'])
    ax1.set_yticklabels(['Loss', 'Win'])
    
    # Plot percentages
    sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', cbar=False, ax=ax2)
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')
    ax2.set_title(f'{title} (Percentages)')
    ax2.set_xticklabels(['Loss', 'Win'])
    ax2.set_yticklabels(['Loss', 'Win'])
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved confusion matrix to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def plot_feature_importance(importances: Dict[str, float], top_n: int = 20, 
                           output_path: Optional[Union[str, Path]] = None,
                           progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save feature importance.
    
    Args:
        importances: Dictionary of feature importances
        top_n: Number of top features to plot
        output_path: Path to save the plot
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting top {top_n} feature importances")
    
    # Sort and limit to top N
    sorted_importances = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:top_n])
    
    # Calculate total importance for normalization
    total_importance = sum(sorted_importances.values())
    normalized_importances = {k: v/total_importance*100 for k, v in sorted_importances.items()}
    
    # Reverse order for horizontal bar chart (to have highest at top)
    features = list(reversed(list(normalized_importances.keys())))
    values = list(reversed(list(normalized_importances.values())))
    
    # Plot
    plt.figure(figsize=(12, 10))
    
    # Plot horizontal bar chart with percentages
    bars = plt.barh(features, values, color='cornflowerblue')
    
    # Add percentage labels
    for i, bar in enumerate(bars):
        plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, f'{values[i]:.1f}%',
                va='center', fontsize=10)
    
    plt.xlabel('Relative Importance (%)')
    plt.title(f'Top {top_n} Feature Importances')
    plt.gca().spines[['right', 'top']].set_visible(False)  # Clean up
    plt.grid(axis='x', linestyle='--', alpha=0.6)
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved feature importance plot to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update("Feature importance plotting complete")


def plot_roc_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                  output_path: Optional[Union[str, Path]] = None,
                  title: str = "ROC Curve",
                  progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save ROC curve.
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    
    # Plot thresholds
    threshold_markers = [0.1, 0.3, 0.5, 0.7, 0.9]
    for threshold in threshold_markers:
        # Find closest threshold
        idx = (np.abs(thresholds - threshold)).argmin()
        plt.plot(fpr[idx], tpr[idx], 'o', markersize=8, 
                label=f'Threshold = {thresholds[idx]:.2f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.grid(alpha=0.3)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved ROC curve to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def plot_precision_recall_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                              output_path: Optional[Union[str, Path]] = None,
                              title: str = "Precision-Recall Curve",
                              progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save precision-recall curve.
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    # Calculate PR curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
    pr_auc = average_precision_score(y_true, y_pred_proba)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.plot(recall, precision, color='green', lw=2, label=f'PR curve (AP = {pr_auc:.3f})')
    
    # Add baseline
    baseline = sum(y_true) / len(y_true)
    plt.axhline(y=baseline, color='navy', lw=2, linestyle='--', label=f'Baseline (= {baseline:.3f})')
    
    # Plot thresholds
    threshold_markers = [0.1, 0.3, 0.5, 0.7, 0.9]
    for threshold in threshold_markers:
        if len(thresholds) > 0:  # Check if we have thresholds
            # Find closest threshold (handling edge case)
            idx = min(len(thresholds) - 1, (np.abs(thresholds - threshold)).argmin())
            idx2 = min(len(precision) - 1, idx)  # Ensure we don't exceed precision length
            plt.plot(recall[idx2], precision[idx2], 'o', markersize=8, 
                    label=f'Threshold = {thresholds[idx]:.2f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title)
    plt.legend(loc='lower left')
    plt.grid(alpha=0.3)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved precision-recall curve to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def plot_calibration_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                          output_path: Optional[Union[str, Path]] = None,
                          title: str = "Calibration Curve",
                          progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save calibration curve.
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    # Calculate calibration curve
    prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=10)
    
    # Calculate Brier score
    brier = brier_score_loss(y_true, y_pred_proba)
    
    # Plot
    plt.figure(figsize=(10, 8))
    
    # Plot calibration curve
    plt.plot(prob_pred, prob_true, 's-', color='darkgreen', lw=2, 
            label=f'Calibration curve (Brier score = {brier:.3f})')
    
    # Plot perfect calibration
    plt.plot([0, 1], [0, 1], '--', color='gray', label='Perfect calibration')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('Mean predicted probability')
    plt.ylabel('Fraction of positives (Empirical probability)')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.grid(alpha=0.3)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved calibration curve to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def save_metrics(metrics: Dict[str, Any], output_path: Union[str, Path],
                progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Save evaluation metrics to JSON.
    
    Args:
        metrics: Dictionary of metrics
        output_path: Path to save metrics
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Saving metrics to {output_path}")
    
    # Convert numpy types to Python types for JSON serialization
    def convert_numpy_types(obj):
        if isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_numpy_types(x) for x in obj]
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return convert_numpy_types(obj.tolist())
        elif isinstance(obj, datetime):
            return obj.isoformat()
        elif obj is None:
            return None
        else:
            return obj
    
    metrics_json = convert_numpy_types(metrics)
    
    # Add timestamp
    metrics_json['timestamp'] = datetime.now().isoformat()
    
    # Add model information
    metrics_json['model_info'] = {
        'model_path': str(MODEL_OUTPUT),
        'feature_count': metrics_json.get('feature_count', None),
        'data_size': metrics_json.get('data_size', None)
    }
    
    # Save to file
    with open(output_path, 'w') as f:
        json.dump(metrics_json, f, indent=2)
    
    logger.info(f"Metrics saved to {output_path}")
    
    if progress_tracker:
        progress_tracker.update("Metrics saving complete")


def evaluate_player_position_bias(model: xgb.Booster, test_df: pd.DataFrame, feature_cols: List[str],
                                 progress_tracker: Optional[ProgressTracker] = None) -> Dict[str, float]:
    """
    Evaluate if the model has any bias based on player position.
    
    Args:
        model: Trained XGBoost model
        test_df: Test DataFrame
        feature_cols: List of feature column names
        progress_tracker: Optional progress tracker
        
    Returns:
        Dictionary with bias metrics
    """
    logger.info("Evaluating player position bias")
    
    # Get unique match IDs
    match_ids = test_df['match_id'].unique()
    
    # Initialize counters
    same_prediction_count = 0
    different_prediction_count = 0
    
    # Create a progress bar
    pbar = tqdm(match_ids, desc="Checking player position bias")
    
    for match_id in pbar:
        # Get the two rows for this match (player1 as winner and player1 as loser)
        match_rows = test_df[test_df['match_id'] == match_id]
        
        if len(match_rows) != 2:
            continue
        
        # Get features for both scenarios
        X1 = match_rows.iloc[0][feature_cols].values.reshape(1, -1)
        X2 = match_rows.iloc[1][feature_cols].values.reshape(1, -1)
        
        # Create DMatrix objects
        dmat1 = xgb.DMatrix(X1, feature_names=feature_cols)
        dmat2 = xgb.DMatrix(X2, feature_names=feature_cols)
        
        # Get predictions
        pred1 = model.predict(dmat1)[0]
        pred2 = model.predict(dmat2)[0]
        
        # Check if predictions are consistent
        # pred1 > 0.5 should mean pred2 < 0.5 (and vice versa)
        # If both are on the same side of 0.5, we have inconsistency
        if (pred1 > 0.5 and pred2 > 0.5) or (pred1 < 0.5 and pred2 < 0.5):
            different_prediction_count += 1
        else:
            same_prediction_count += 1
    
    # Calculate consistency percentage
    total_matches = same_prediction_count + different_prediction_count
    if total_matches > 0:
        consistency_pct = (same_prediction_count / total_matches) * 100
    else:
        consistency_pct = 0
    
    logger.info(f"Player position consistency: {consistency_pct:.2f}%")
    logger.info(f"Consistent predictions: {same_prediction_count} / {total_matches}")
    logger.info(f"Inconsistent predictions: {different_prediction_count} / {total_matches}")
    
    bias_metrics = {
        'player_position_consistency_pct': consistency_pct,
        'consistent_predictions': same_prediction_count,
        'inconsistent_predictions': different_prediction_count,
        'total_matches': total_matches
    }
    
    if progress_tracker:
        progress_tracker.update("Player position bias analysis complete")
    
    return bias_metrics


def select_features_by_importance(
    importances: dict[str, float], 
    threshold: float = 0.95, 
    min_features: int = 20
) -> list[str]:
    """
    Select features based on cumulative importance up to a threshold.
    
    Args:
        importances: Dictionary of feature importance scores
        threshold: Cumulative importance threshold (0.0-1.0)
        min_features: Minimum number of features to select
        
    Returns:
        List of selected feature names
    """
    # Sort features by importance
    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)
    
    # Calculate total importance
    total_importance = sum(importances.values())
    
    # Select features up to threshold
    selected_features = []
    cumulative_importance = 0.0
    
    for feature, importance in sorted_features:
        selected_features.append(feature)
        cumulative_importance += importance / total_importance
        
        if cumulative_importance >= threshold and len(selected_features) >= min_features:
            break
    
    # Ensure minimum number of features
    if len(selected_features) < min_features:
        remaining = [f for f, _ in sorted_features if f not in selected_features]
        selected_features.extend(remaining[:min_features - len(selected_features)])
    
    logger.info(f"Selected {len(selected_features)} features covering {cumulative_importance:.2%} of total importance")
    return selected_features


def save_metrics(metrics: dict, output_file: Path, progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Save evaluation metrics to a JSON file.
    
    Args:
        metrics: Dictionary of metrics
        output_file: Path to save metrics
        progress_tracker: Optional progress tracker
    """
    # Convert numpy types to Python native types for JSON serialization
    def convert_to_serializable(obj):
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(i) for i in obj]
        else:
            return obj
    
    serializable_metrics = convert_to_serializable(metrics)
    
    with open(output_file, 'w') as f:
        json.dump(serializable_metrics, f, indent=4)
    
    if progress_tracker:
        progress_tracker.update("Metrics saved")


def plot_feature_importance(
    importances: dict[str, float], 
    top_n: int, 
    output_file: Path,
    progress_tracker: Optional[ProgressTracker] = None
) -> None:
    """
    Plot feature importance and save to file.
    
    Args:
        importances: Dictionary of feature importance scores
        top_n: Number of top features to display
        output_file: Path to save the plot
        progress_tracker: Optional progress tracker
    """
    plt.figure(figsize=(12, 8))
    
    # Get top features
    top_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:top_n]
    features, scores = zip(*top_features)
    
    # Create horizontal bar chart
    y_pos = np.arange(len(features))
    plt.barh(y_pos, scores, align='center')
    plt.yticks(y_pos, [f[:30] + '...' if len(f) > 30 else f for f in features])
    plt.xlabel('Importance Score')
    plt.title(f'Top {top_n} Feature Importance')
    plt.tight_layout()
    
    # Save the plot
    plt.savefig(output_file, dpi=300)
    plt.close()
    
    if progress_tracker:
        progress_tracker.update("Feature importance plot created")


def plot_roc_curve(y_true: np.ndarray, y_pred_proba: np.ndarray, output_file: Path) -> None:
    """
    Plot ROC curve and save to file.
    
    Args:
        y_true: True binary labels
        y_pred_proba: Predicted probabilities
        output_file: Path to save the plot
    """
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_precision_recall_curve(y_true: np.ndarray, y_pred_proba: np.ndarray, output_file: Path) -> None:
    """
    Plot Precision-Recall curve and save to file.
    
    Args:
        y_true: True binary labels
        y_pred_proba: Predicted probabilities
        output_file: Path to save the plot
    """
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    pr_auc = average_precision_score(y_true, y_pred_proba)
    
    plt.figure(figsize=(10, 8))
    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.3f})')
    plt.axhline(y=sum(y_true)/len(y_true), color='red', linestyle='--', label=f'Baseline (positive rate = {sum(y_true)/len(y_true):.3f})')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.grid(True, alpha=0.3)
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_calibration_curve(y_true: np.ndarray, y_pred_proba: np.ndarray, output_file: Path) -> None:
    """
    Plot calibration curve and save to file.
    
    Args:
        y_true: True binary labels
        y_pred_proba: Predicted probabilities
        output_file: Path to save the plot
    """
    plt.figure(figsize=(10, 8))
    
    # Calculate calibration curve
    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred_proba, n_bins=10)
    
    # Plot perfect calibration
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    
    # Plot model calibration
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Model")
    
    # Calculate and display Brier score
    brier = brier_score_loss(y_true, y_pred_proba)
    plt.title(f'Calibration Curve (Brier Score: {brier:.3f})')
    
    plt.xlabel('Mean predicted probability')
    plt.ylabel('Fraction of positives')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, output_file: Path, title: str) -> None:
    """
    Plot confusion matrix and save to file.
    
    Args:
        y_true: True binary labels
        y_pred: Predicted binary labels
        output_file: Path to save the plot
        title: Title for the plot
    """
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 8))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    
    tick_marks = np.arange(2)
    plt.xticks(tick_marks, ['Loser', 'Winner'])
    plt.yticks(tick_marks, ['Loser', 'Winner'])
    
    # Add text annotations
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
    
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_shap_values(model: xgb.Booster, X: np.ndarray, feature_names: list[str], output_file: Path) -> None:
    """
    Plot SHAP values for feature explanation and save to file.
    
    Args:
        model: Trained XGBoost model
        X: Feature matrix
        feature_names: List of feature names
        output_file: Path to save the plot
    """
    try:
        import shap
    except ImportError:
        logger.warning("SHAP package not installed. Skipping SHAP plot.")
        return
    
    # Create DMatrix
    dmatrix = xgb.DMatrix(X, feature_names=feature_names)
    
    # Get SHAP values
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(dmatrix)
    
    # Create summary plot
    plt.figure(figsize=(10, 12))
    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
    plt.tight_layout()
    
    # Save the plot
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()


def save_pipeline(
    model: xgb.Booster, 
    features: list[str], 
    feature_transformer: Optional[Any],
    output_file: Path,
    progress_tracker: Optional[ProgressTracker] = None
) -> None:
    """
    Save the model pipeline for easy prediction.
    
    Args:
        model: Trained XGBoost model
        features: List of feature names
        feature_transformer: Any preprocessing transformer (optional)
        output_file: Path to save the pipeline
        progress_tracker: Optional progress tracker
    """
    pipeline = {
        'model': model,
        'features': features,
        'feature_transformer': feature_transformer,
        'metadata': {
            'creation_date': datetime.now().isoformat(),
            'model_type': 'xgboost',
            'feature_count': len(features),
            'version': '3.0'
        }
    }
    
    with open(output_file, 'wb') as f:
        pickle.dump(pipeline, f)
    
    if progress_tracker:
        progress_tracker.update("Pipeline saved")


def main() -> None:
    """
    Main function for training tennis prediction model.
    """
    start_time = time.time()
    
    # Create output directories if they don't exist
    os.makedirs(MODELS_DIR, exist_ok=True)
    os.makedirs(PLOTS_DIR, exist_ok=True)
    
    # Log hardware information and model configuration
    logger.info(f"XGBoost version: {xgb.__version__}")
    logger.info(f"Python version: {sys.version}")
    logger.info(f"CPU count: {os.cpu_count()}")
    logger.info(f"Memory: {psutil.virtual_memory().total / (1024 ** 3):.1f} GB")
    
    # Check CUDA availability for GPU acceleration
    device_info = detect_optimal_device()
    if device_info.get('tree_method') == 'gpu_hist':
        logger.info("Training will use GPU acceleration (CUDA)")
    else:
        logger.info(f"Training will use CPU with {device_info.get('nthread', 'default')} threads")
    
    # Define the total number of processing steps for progress tracking
    total_steps = 10
    progress_tracker = ProgressTracker(total_steps, "Tennis Model Training")
    
    try:
        # Step 1: Load data from PostgreSQL database
        logger.info(f"Step 1/{total_steps}: Loading data from database...")
        df = load_data_from_database(progress_tracker=progress_tracker)
        logger.info(f"Loaded {len(df)} samples spanning from "
                   f"{df['tournament_date'].min().date()} to {df['tournament_date'].max().date()}")
        
        # Check if we have enough data
        if len(df) < 1000:
            logger.warning(f"Potentially insufficient data: only {len(df)} samples")
        
        # Step 2: Create time-based train/val/test split
        logger.info(f"Step 2/{total_steps}: Creating strict time-based split...")
        train_df, val_df, test_df = create_time_based_train_val_test_split(
            df, TRAIN_SPLIT, VAL_SPLIT, TEST_SPLIT, progress_tracker
        )
        
        # Step 3: Get feature columns
        logger.info(f"Step 3/{total_steps}: Identifying feature columns...")
        feature_cols = get_feature_columns(df, progress_tracker)
        
        # Step 4: Prepare features and labels - no scaling for XGBoost
        logger.info(f"Step 4/{total_steps}: Preparing features and labels...")
        (X_train, X_val, X_test), (y_train, y_val, y_test), categorical_indices = prepare_features(
            train_df, val_df, test_df, feature_cols, progress_tracker
        )
        
        # Step 5: Tune hyperparameters
        logger.info(f"Step 5/{total_steps}: Tuning hyperparameters...")
        best_params = tune_hyperparameters(
            X_train, y_train, X_val, y_val, feature_cols, categorical_indices, progress_tracker
        )
        
        # Step 6: Train model with best parameters
        logger.info(f"Step 6/{total_steps}: Training model with tuned hyperparameters...")
        model, evals_result = train_model(
            X_train, y_train, X_val, y_val, feature_cols, categorical_indices, best_params, 50, progress_tracker
        )
        
        # Step 7: Extract feature importance
        logger.info(f"Step 7/{total_steps}: Extracting feature importance...")
        importance_scores = model.get_score(importance_type='gain')
        importances = {feature: score for feature, score in importance_scores.items() if feature in feature_cols}
        importances = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True))
        
        # Log top features
        top_features = list(importances.items())[:10]
        logger.info("Top 10 features by importance:")
        for feature, importance in top_features:
            logger.info(f"  - {feature}: {importance:.4f}")
        
        # Step 8: Select most important features
        logger.info(f"Step 8/{total_steps}: Selecting top features...")
        selected_features = select_features_by_importance(importances, threshold=0.95, min_features=20)
        
        # Step 9: Retrain with selected features (improves model performance and reduces size)
        logger.info(f"Step 9/{total_steps}: Retraining with selected features...")
        X_train_selected = X_train[:, [feature_cols.index(f) for f in selected_features]]
        X_val_selected = X_val[:, [feature_cols.index(f) for f in selected_features]]
        X_test_selected = X_test[:, [feature_cols.index(f) for f in selected_features]]
        
        # Update categorical indices for selected features
        selected_categorical_indices = []
        for i, feature in enumerate(selected_features):
            if feature_cols.index(feature) in categorical_indices:
                selected_categorical_indices.append(i)
        
        # Retrain
        final_model, final_evals_result = train_model(
            X_train_selected, y_train, X_val_selected, y_val, selected_features, 
            selected_categorical_indices, best_params, 50, progress_tracker
        )
        
        # Step 10: Evaluate model
        logger.info(f"Step 10/{total_steps}: Evaluating model...")
        test_metrics = evaluate_model(
            final_model, X_test_selected, y_test, selected_features, 
            dataset_name="Test",
            threshold=0.5,
            surfaces=SURFACES,
            progress_tracker=progress_tracker
        )
        
        # Also evaluate on validation set to compare
        val_metrics = evaluate_model(
            final_model, X_val_selected, y_val, selected_features, 
            dataset_name="Validation",
            threshold=0.5,
            surfaces=None  # Skip surface-specific metrics for validation
        )
        
        # Check for potential overfitting
        val_acc = val_metrics['overall']['accuracy']
        test_acc = test_metrics['overall']['accuracy']
        acc_diff = abs(val_acc - test_acc)
        
        if acc_diff > 0.05:
            logger.warning(f"Potential overfitting: validation accuracy {val_acc:.4f} vs test accuracy {test_acc:.4f}")
            logger.warning(f"Difference: {acc_diff:.4f} - model may not generalize well to new data")
        else:
            logger.info(f"Model generalizes well: validation accuracy {val_acc:.4f} vs test accuracy {test_acc:.4f}")
        
        # Step 11: Save metrics
        logger.info(f"Step 11/{total_steps}: Saving evaluation metrics...")
        combined_metrics = {
            'test': test_metrics,
            'validation': val_metrics,
            'selected_features': selected_features,
            'feature_importance': {k: float(v) for k, v in importances.items()},
            'training_info': {
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'test_samples': len(X_test),
                'training_date': datetime.now().isoformat(),
                'data_date_range': {
                    'start': df['tournament_date'].min().isoformat(),
                    'end': df['tournament_date'].max().isoformat()
                }
            }
        }
        save_metrics(combined_metrics, METRICS_OUTPUT, progress_tracker)
        
        # Step 12: Save hyperparameters
        logger.info(f"Step 12/{total_steps}: Saving hyperparameters...")
        with open(HYPERPARAMS_OUTPUT, "w") as f:
            json.dump(best_params, f, indent=4)
        progress_tracker.update("Hyperparameters saved")
        
        # Step 13: Plot feature importance
        logger.info(f"Step 13/{total_steps}: Plotting feature importance...")
        plot_feature_importance(
            importances, min(20, len(selected_features)), 
            PLOTS_DIR / "feature_importance.png", progress_tracker
        )
        
        # Also plot ROC and PR curves
        y_pred_proba = final_model.predict(xgb.DMatrix(X_test_selected, feature_names=selected_features))
        plot_roc_curve(y_test, y_pred_proba, PLOTS_DIR / "roc_curve.png")
        plot_precision_recall_curve(y_test, y_pred_proba, PLOTS_DIR / "pr_curve.png")
        plot_calibration_curve(y_test, y_pred_proba, PLOTS_DIR / "calibration_curve.png")
        
        # Step 14: Save model and pipeline
        logger.info(f"Step 14/{total_steps}: Saving model and pipeline...")
        final_model.save_model(str(MODEL_OUTPUT))
        logger.info(f"Model saved to {MODEL_OUTPUT}")
        
        # Save full pipeline for easy prediction
        save_pipeline(final_model, selected_features, None, PIPELINE_OUTPUT, progress_tracker)
        
        # Generate confusion matrix plot
        y_pred = (y_pred_proba >= 0.5).astype(int)
        plot_confusion_matrix(y_test, y_pred, PLOTS_DIR / "confusion_matrix_test.png", "Test Set Confusion Matrix")
        
        # Print final message
        total_time = time.time() - start_time
        logger.info(f"Model training completed in {total_time:.2f} seconds")
        logger.info(f"Model accuracy on test set: {test_metrics['overall']['accuracy']:.4f}")
        logger.info(f"Model saved to: {MODEL_OUTPUT}")
        logger.info(f"Pipeline saved to: {PIPELINE_OUTPUT}")
        logger.info(f"Metrics saved to: {METRICS_OUTPUT}")
        
        # Plot SHAP values for feature explanation if data size is manageable
        if len(X_test_selected) <= 1000:  # Limit for SHAP analysis to avoid memory issues
            sample_size = min(500, len(X_test_selected))  # Further limit sample size
            sample_indices = np.random.choice(len(X_test_selected), sample_size, replace=False)
            logger.info(f"Generating SHAP plots with {sample_size} samples...")
            plot_shap_values(
                final_model, 
                X_test_selected[sample_indices], 
                selected_features, 
                PLOTS_DIR / "shap_summary.png"
            )
        else:
            logger.info("Skipping SHAP plot generation due to large test set size")
        
    except Exception as e:
        logger.error(f"Error in training process: {str(e)}")
        logger.exception("Exception details:")
        raise


if __name__ == "__main__":
    main() 