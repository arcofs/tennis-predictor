#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Tennis Match Prediction Model Training Script - Version 3

This script trains an XGBoost model to predict tennis match outcomes
using features generated by generate_features_v3.py.

The script follows best practices for XGBoost model training:
- Strict time-based train/val/test split to prevent data leakage
- Hyperparameter tuning with Optuna
- Proper handling of categorical features
- Feature selection based on importance
- Comprehensive model evaluation
- Model and pipeline saving for inference
"""

import os
import sys
from pathlib import Path

# Add project root to Python path
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import json
import time
import pickle
import logging
import warnings
import argparse
from datetime import datetime
from typing import Optional, Any, Dict, List, Tuple, Union, cast

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import xgboost as xgb
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances
import psycopg2
from tqdm import tqdm
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, roc_curve, auc,
    precision_recall_curve, log_loss, brier_score_loss,
    confusion_matrix, balanced_accuracy_score, classification_report
)
from sklearn.calibration import calibration_curve
from psycopg2 import pool
from dotenv import load_dotenv
import psutil
from predictor.v3.data_cache import get_cache_key, get_cached_data, save_to_cache

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# Define a custom exception for optimization failures
class OptimizationFailedError(Exception):
    """Error indicating a fundamental problem with optimization that should not be counted as a trial."""
    pass

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("train_model_v3.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("tennis_predictor")

# Project directories
PROJECT_DIR = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_DIR / "data"
OUTPUT_DIR = PROJECT_DIR / "output" / "v3"
MODELS_DIR = OUTPUT_DIR / "models"
PLOTS_DIR = OUTPUT_DIR / "plots"

# Input/output file paths
INPUT_FILE = DATA_DIR / "processed" / "tennis_matches_features_v3.csv"
MODEL_OUTPUT = MODELS_DIR / "tennis_model_v3.json"
PIPELINE_OUTPUT = MODELS_DIR / "tennis_pipeline_v3.pkl"
METRICS_OUTPUT = OUTPUT_DIR / "model_metrics_v3.json"
HYPERPARAMS_OUTPUT = OUTPUT_DIR / "hyperparameters_v3.json"
OPTUNA_STUDY_OUTPUT = OUTPUT_DIR / "optuna_study_v3.pkl"

# Training configuration
TRAIN_SPLIT = 0.7
VAL_SPLIT = 0.15
TEST_SPLIT = 0.15
RANDOM_SEED = 42
TARGET_COLUMN = 'result'
ID_COLUMNS = ['player1_id', 'player2_id', 'tournament_id', 'match_id']
DATETIME_COLUMNS = ['tournament_date']
CATEGORICAL_COLUMNS_PATTERN = [
    'surface', 'tournament_level', 'round', 'court_type', 
    'tournament_name', 'player1_hand', 'player2_hand'
]

# Tennis surfaces for surface-specific evaluation
SURFACES = ["Hard", "Clay", "Grass", "Carpet"]

# Database configuration
DB_BATCH_SIZE = 10000  # Number of records to fetch in each database batch
DB_TIMEOUT_SECONDS = 30  # Database query timeout in seconds

# Default CPU threads to use if GPU is not available
DEFAULT_CPU_THREADS = max(4, os.cpu_count() - 2) if os.cpu_count() else 4

def detect_optimal_device() -> dict:
    """
    Detect the optimal device for XGBoost training.
    Tries GPU first with CUDA, then falls back to CPU with multithreading.
    
    Returns:
        Dict with optimal parameters for tree_method and nthread
    """
    # Try GPU first
    try:
        # Create small test matrix to verify GPU works
        test_matrix = xgb.DMatrix(np.array([[1, 2], [3, 4]]), label=np.array([0, 1]))
        test_params = {'tree_method': 'gpu_hist'}
        xgb.train(test_params, test_matrix, num_boost_round=1)
        
        logger.info("CUDA GPU acceleration available and will be used")
        return {
            'tree_method': 'gpu_hist',
            'gpu_id': 0
        }
    except Exception as e:
        logger.info(f"GPU acceleration not available ({str(e)}), falling back to CPU with multithreading")
        
        # Determine optimal number of threads for CPU
        cpu_threads = DEFAULT_CPU_THREADS
        logger.info(f"Using {cpu_threads} CPU threads for XGBoost training")
        
        return {
            'tree_method': 'hist',
            'nthread': cpu_threads
        }

# XGBoost baseline parameters with optimal device detection
device_params = detect_optimal_device()
BASELINE_PARAMS = {
    'objective': 'binary:logistic',
    'eval_metric': ['logloss', 'error', 'auc'],
    'grow_policy': 'lossguide',  # Can improve accuracy
    'max_depth': 6,
    'min_child_weight': 1,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': RANDOM_SEED,
    'verbosity': 0
}
# Add device-specific parameters
BASELINE_PARAMS.update(device_params)

def get_database_connection() -> psycopg2.extensions.connection:
    """
    Create a database connection using environment variables.
    
    Returns:
        Database connection
    """
    # Load environment variables
    load_dotenv()
    
    # Get database URL from environment
    database_url = os.environ.get("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL not found in environment variables")
    
    try:
        # Convert postgres:// to postgresql:// if needed (psycopg2 requirement)
        if database_url.startswith('postgres://'):
            database_url = database_url.replace('postgres://', 'postgresql://', 1)
            
        # Add connection timeout and statement timeout settings
        connection = psycopg2.connect(
            database_url,
            connect_timeout=10,
            options=f"-c statement_timeout={DB_TIMEOUT_SECONDS * 1000}"
        )
        logger.info("Successfully connected to database")
        return connection
    except Exception as e:
        logger.error(f"Error connecting to database: {e}")
        raise

def load_data_from_database(limit: Optional[int] = None, 
                           progress_tracker: Optional['ProgressTracker'] = None) -> pd.DataFrame:
    """
    Load the tennis match features from the database efficiently.
    Handles the match_features table created by generate_features_v3.py.
    Uses caching to avoid unnecessary database calls.
    
    Args:
        limit: Optional limit on number of rows to fetch
        progress_tracker: Optional progress tracker
        
    Returns:
        DataFrame with tennis match features
    """
    # Define the base query
    base_query = """
    SELECT *
    FROM match_features
    ORDER BY tournament_date ASC
    """
    
    # Generate cache key
    cache_key = get_cache_key(base_query, limit)
    
    # Try to load from cache first
    cached_df = get_cached_data(cache_key)
    if cached_df is not None:
        logger.info("Using cached data")
        if progress_tracker:
            progress_tracker.update("Data loaded from cache")
        return cached_df
    
    # If not in cache, load from database
    logger.info("Loading data from database (match_features table)...")
    
    # Connect to database
    conn = get_database_connection()
    
    try:
        # Define the query with dynamic row limit
        limit_clause = f"LIMIT {limit}" if limit else ""
        
        # Use batched loading to handle large datasets efficiently
        offset = 0
        dataframes = []
        total_rows = 0
        
        if limit:
            total_to_fetch = limit
        else:
            # Get total count first
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM match_features")
                total_to_fetch = cursor.fetchone()[0]
        
        logger.info(f"Fetching up to {total_to_fetch} rows from database")
        pbar = tqdm(total=total_to_fetch, desc="Loading data from database")
        
        while True:
            # Define batch query
            query = f"""
            {base_query}
            {limit_clause}
            OFFSET {offset}
            LIMIT {DB_BATCH_SIZE}
            """
            
            # Load batch
            batch_df = pd.read_sql(query, conn)
            
            # If batch is empty, we're done
            if len(batch_df) == 0:
                break
                
            # Append to list of dataframes
            dataframes.append(batch_df)
            
            # Update counts
            rows_fetched = len(batch_df)
            total_rows += rows_fetched
            pbar.update(rows_fetched)
            
            # Check if we've reached the limit
            if limit and total_rows >= limit:
                break
                
            # Update offset for next batch
            offset += DB_BATCH_SIZE
        
        pbar.close()
        
        # Combine all batches
        if dataframes:
            df = pd.concat(dataframes, ignore_index=True)
            
            # Convert date columns to datetime
            if 'tournament_date' in df.columns:
                df['tournament_date'] = pd.to_datetime(df['tournament_date'])
            
            # Handle column name mappings to maintain compatibility with the code
            column_mapping = {
                'player1_id': 'player1_id',
                'player2_id': 'player2_id'
            }
            
            # Rename columns if needed
            df = df.rename(columns=column_mapping)
            
            # Ensure result is an integer (1 for win, 0 for loss)
            if 'result' in df.columns:
                df['result'] = df['result'].astype(int)
            
            # Sort by date
            df = df.sort_values(by='tournament_date').reset_index(drop=True)
            
            logger.info(f"Loaded {len(df)} match features")
            logger.info(f"Feature columns: {len(df.columns)} columns including date, IDs, etc.")
            
            # Log information about key features
            numerical_cols = [col for col in df.columns if df[col].dtype.kind in 'fib' 
                             and col not in ['match_id', 'player1_id', 'player2_id', 'result']]
            logger.info(f"Number of numerical features: {len(numerical_cols)}")
            
            # Log sample values
            if len(df) > 0:
                logger.info(f"Time span: {df['tournament_date'].min()} to {df['tournament_date'].max()}")
            
            # Save to cache for future use
            save_to_cache(df, cache_key)
            
            if progress_tracker:
                progress_tracker.update("Data loading complete")
            
            return df
        else:
            logger.warning("No data retrieved from database")
            return pd.DataFrame()
    
    finally:
        conn.close()

# Note: Only database loading is used in this workflow.
# File-based loading has been removed as it was not being used.

class ProgressTracker:
    """Helper class to track and log progress during model training."""
    
    def __init__(self, total_steps: int, task_description: str):
        """
        Initialize the progress tracker.
        
        Args:
            total_steps: Total number of steps in the process
            task_description: Description of the task
        """
        self.total_steps = total_steps
        self.current_step = 0
        self.task_description = task_description
        self.start_time = time.time()
        self.last_update_time = self.start_time
        
        logger.info(f"Starting {task_description} with {total_steps} steps")
    
    def update(self, step_description: str) -> None:
        """
        Update the progress tracker.
        
        Args:
            step_description: Description of the current step
        """
        self.current_step += 1
        current_time = time.time()
        elapsed = current_time - self.last_update_time
        total_elapsed = current_time - self.start_time
        
        logger.info(f"[{self.current_step}/{self.total_steps}] {step_description} "
                   f"(step: {elapsed:.2f}s, total: {total_elapsed:.2f}s)")
        
        self.last_update_time = current_time
    
    def get_progress(self) -> float:
        """
        Get the current progress as a fraction.
        
        Returns:
            Progress as a float between 0 and 1
        """
        return self.current_step / self.total_steps


class XGBoostProgressCallback(xgb.callback.TrainingCallback):
    """
    Callback to track XGBoost training progress.
    """
    def __init__(self, total_rounds: int):
        self.total_rounds = total_rounds
        self.current_round = 0
        self.start_time = time.time()
        
    def after_iteration(self, model, epoch, evals_log):
        """Log progress after each iteration."""
        self.current_round += 1
        if self.current_round % 10 == 0 or self.current_round == self.total_rounds:
            elapsed = time.time() - self.start_time
            progress = (self.current_round / self.total_rounds) * 100
            
            if self.current_round < self.total_rounds:
                est_remaining = (elapsed / self.current_round) * (self.total_rounds - self.current_round)
                logger.info(f"XGBoost training: {progress:.1f}% complete. Est. remaining time: {est_remaining:.1f}s")
            else:
                logger.info(f"XGBoost training: 100% complete. Total time: {elapsed:.1f}s")
                
        return False  # Continue training


def create_time_based_train_val_test_split(
    df: pd.DataFrame,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    progress_tracker: Optional[ProgressTracker] = None
) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Create a strict time-based split of the data to prevent data leakage.
    
    Args:
        df: DataFrame containing the tennis match data with features
        train_ratio: Proportion of data to use for training
        val_ratio: Proportion of data to use for validation
        test_ratio: Proportion of data to use for testing
        progress_tracker: Optional progress tracker
        
    Returns:
        Tuple of (train_df, val_df, test_df)
    """
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-10:
        raise ValueError(f"Split ratios must sum to 1.0, got {train_ratio + val_ratio + test_ratio}")
    
    # Sort by date to ensure chronological ordering
    df_sorted = df.sort_values('tournament_date')
    
    # Calculate split indices
    n = len(df_sorted)
    train_end = int(n * train_ratio)
    val_end = train_end + int(n * val_ratio)
    
    # Create splits
    train_df = df_sorted.iloc[:train_end].copy()
    val_df = df_sorted.iloc[train_end:val_end].copy()
    test_df = df_sorted.iloc[val_end:].copy()
    
    # Verify time-based integrity of splits
    train_max_date = train_df['tournament_date'].max()
    val_min_date = val_df['tournament_date'].min()
    val_max_date = val_df['tournament_date'].max()
    test_min_date = test_df['tournament_date'].min()
    
    if val_min_date < train_max_date or test_min_date < val_max_date:
        logger.warning("Time-based split integrity issue detected!")
        logger.warning(f"Training max date: {train_max_date.date()}")
        logger.warning(f"Validation min date: {val_min_date.date()}")
        logger.warning(f"Validation max date: {val_max_date.date()}")
        logger.warning(f"Test min date: {test_min_date.date()}")
    
    # Log the split information
    logger.info(f"Training data: {len(train_df)} samples from {train_df['tournament_date'].min().date()} to {train_df['tournament_date'].max().date()}")
    logger.info(f"Validation data: {len(val_df)} samples from {val_df['tournament_date'].min().date()} to {val_df['tournament_date'].max().date()}")
    logger.info(f"Test data: {len(test_df)} samples from {test_df['tournament_date'].min().date()} to {test_df['tournament_date'].max().date()}")
    
    if progress_tracker:
        progress_tracker.update("Time-based split created")
    
    return train_df, val_df, test_df


def get_feature_columns(df: pd.DataFrame, progress_tracker: Optional[ProgressTracker] = None) -> list[str]:
    """
    Identify feature columns for training.
    
    Args:
        df: DataFrame containing tennis match data with features
        progress_tracker: Optional progress tracker
        
    Returns:
        List of feature column names
    """
    logger.info("Identifying feature columns...")
    
    # Exclude non-feature columns
    exclude_columns = [
        'match_id', 'tournament_date', 'tournament_id', 'player1_id', 'player2_id', 
        'player1_name', 'player2_name', 'result', 'created_at', 'updated_at'
    ]
    
    # Get all column names 
    all_columns = df.columns.tolist()
    
    # Filter out non-feature columns
    feature_cols = [col for col in all_columns if col not in exclude_columns]
    
    # Ensure all features are numeric or categorical
    numeric_features = []
    categorical_features = []
    
    for col in feature_cols:
        if df[col].dtype == 'object' or df[col].dtype.name == 'category':
            categorical_features.append(col)
        elif pd.api.types.is_numeric_dtype(df[col]):
            numeric_features.append(col)
        else:
            logger.warning(f"Removing non-numeric, non-categorical feature: {col}")
            feature_cols.remove(col)
    
    # Log feature types
    elo_features = [col for col in numeric_features if 'elo' in col.lower()]
    win_rate_features = [col for col in numeric_features if 'win_rate' in col.lower()]
    streak_features = [col for col in numeric_features if 'streak' in col.lower()]
    serve_features = [col for col in numeric_features if any(term in col.lower() for term in 
                                                      ['serve', 'ace', 'bp_saved'])]
    return_features = [col for col in numeric_features if any(term in col.lower() for term in 
                                                       ['return', 'bp_conversion'])]
    surface_features = [col for col in numeric_features if any(surface in col.lower() for surface in 
                                                       ['hard', 'clay', 'grass', 'carpet'])]
    
    # Log feature counts by category
    logger.info(f"Total features: {len(feature_cols)}")
    logger.info(f"Numeric features: {len(numeric_features)}")
    logger.info(f"Categorical features: {len(categorical_features)}")
    logger.info(f"Elo features: {len(elo_features)}")
    logger.info(f"Win rate features: {len(win_rate_features)}")
    logger.info(f"Streak features: {len(streak_features)}")
    logger.info(f"Serve features: {len(serve_features)}")
    logger.info(f"Return features: {len(return_features)}")
    logger.info(f"Surface-specific features: {len(surface_features)}")
    
    # Display categorical features
    if categorical_features:
        logger.info(f"Categorical features: {categorical_features}")
    
    if progress_tracker:
        progress_tracker.update("Feature columns identified")
    
    return feature_cols


def prepare_features(
    train_df: pd.DataFrame, 
    val_df: pd.DataFrame, 
    test_df: pd.DataFrame, 
    feature_cols: List[str],
    progress_tracker: Optional[ProgressTracker] = None
) -> Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray], List[int]]:
    """
    Prepare feature matrices and labels for training, validation, and testing.
    Following XGBoost best practices:
    - Let XGBoost handle missing values by keeping NaNs
    - Identify categorical features but leave them as is
    - No scaling needed for tree-based models
    
    Args:
        train_df: Training data
        val_df: Validation data
        test_df: Test data
        feature_cols: List of feature columns
        progress_tracker: Optional progress tracker
        
    Returns:
        Tuple of:
        - (X_train, X_val, X_test): Feature matrices
        - (y_train, y_val, y_test): Target vectors
        - categorical_feature_indices: Indices of categorical features
    """
    logger.info("Preparing features following XGBoost best practices")
    
    # Check for potential data leakage by ensuring:
    # 1. All features are from past matches only (should be handled by generate_features_v3.py)
    # 2. No future information is used in features
    
    # Verify time-based split to confirm no leakage
    train_end_date = train_df['tournament_date'].max()
    val_start_date = val_df['tournament_date'].min()
    val_end_date = val_df['tournament_date'].max()
    test_start_date = test_df['tournament_date'].min()
    
    if not (train_end_date < val_start_date and val_end_date < test_start_date):
        logger.warning(f"Potential data leakage in time-based split: train ends {train_end_date}, "
                      f"val starts {val_start_date}, val ends {val_end_date}, test starts {test_start_date}")
        logger.warning("This may be due to multiple matches on the same day.")
    
    # Identify categorical features
    categorical_features = []
    categorical_indices = []
    
    for i, col in enumerate(feature_cols):
        # Check if column is categorical
        if col in train_df.columns and (
            train_df[col].dtype.name == 'category' or 
            (train_df[col].dtype == 'object' and train_df[col].nunique() < 100)
        ):
            categorical_features.append(col)
            categorical_indices.append(i)
    
    if categorical_features:
        logger.info(f"Found {len(categorical_features)} categorical features: {categorical_features}")
    
    # Check for datetime or non-numeric columns in feature_cols
    non_numeric_cols = []
    for col in feature_cols:
        if col in train_df.columns:
            if pd.api.types.is_datetime64_any_dtype(train_df[col]):
                logger.warning(f"Column {col} is a datetime type and needs to be converted")
                non_numeric_cols.append(col)
            elif not pd.api.types.is_numeric_dtype(train_df[col]) and col not in categorical_features:
                logger.warning(f"Column {col} is not numeric type: {train_df[col].dtype}")
                non_numeric_cols.append(col)
    
    # Filter out any non-numeric and non-categorical features
    if non_numeric_cols:
        logger.warning(f"Removing {len(non_numeric_cols)} non-numeric and non-categorical features: {non_numeric_cols}")
        feature_cols = [col for col in feature_cols if col not in non_numeric_cols]
        # Update categorical indices after removing columns
        categorical_indices = [i for i, col in enumerate(feature_cols) if col in categorical_features]
    
    # Convert categorical features to appropriate format for XGBoost
    # Either convert to codes or leave as-is depending on features
    train_features = train_df[feature_cols].copy()
    val_features = val_df[feature_cols].copy()
    test_features = test_df[feature_cols].copy()
    
    # Handle categorical features specially
    for col in categorical_features:
        # For each categorical feature, ensure it's properly encoded
        # This addresses both string and categorical dtype columns
        if col in train_features.columns and (train_df[col].dtype == 'object' or train_df[col].dtype.name == 'category'):
            # Create category mapping from training data
            categories = train_df[col].astype('category').cat.categories
            
            # Apply consistent encoding across train/val/test
            train_features[col] = train_df[col].astype('category').cat.codes
            
            # For validation and test, handle new categories not seen in training
            val_features[col] = pd.Categorical(
                val_df[col], 
                categories=categories
            ).codes
            
            test_features[col] = pd.Categorical(
                test_df[col], 
                categories=categories
            ).codes
            
            # Replace -1 (unknown category) with NaN for XGBoost to handle
            train_features[col] = train_features[col].replace(-1, np.nan)
            val_features[col] = val_features[col].replace(-1, np.nan)
            test_features[col] = test_features[col].replace(-1, np.nan)
    
    # Ensure all features are numeric - convert any remaining non-numeric types
    for col in feature_cols:
        if col in train_features.columns and not pd.api.types.is_numeric_dtype(train_features[col]):
            logger.warning(f"Converting non-numeric feature {col} to float type")
            train_features[col] = pd.to_numeric(train_features[col], errors='coerce')
            val_features[col] = pd.to_numeric(val_features[col], errors='coerce')
            test_features[col] = pd.to_numeric(test_features[col], errors='coerce')
    
    # Extract features and labels as numpy arrays
    X_train = train_features.values.astype(np.float32)  # Ensure numeric type for XGBoost
    y_train = train_df['result'].values
    
    X_val = val_features.values.astype(np.float32)
    y_val = val_df['result'].values
    
    X_test = test_features.values.astype(np.float32)
    y_test = test_df['result'].values
    
    # Check for any NaN or infinite values
    if np.isnan(X_train).any() or np.isinf(X_train).any():
        logger.warning("Training data contains NaN or infinite values. XGBoost will handle these.")
    
    # Check for features with excessive missing values (>50%)
    # Generate_features_v3.py already handles most missing values, but we check anyway
    missing_pct = np.zeros(X_train.shape[1])
    for i in range(X_train.shape[1]):
        # Use pandas to safely check for NaN values regardless of data type
        col_data = pd.Series(X_train[:, i])
        missing_pct[i] = (col_data.isna().sum() / len(col_data)) * 100
    
    high_missing_features = [(feature_cols[i], pct) for i, pct in enumerate(missing_pct) if pct > 50]
    
    if high_missing_features:
        logger.warning(f"Features with >50% missing values (potential quality issues):")
        for feature, pct in high_missing_features:
            logger.warning(f"  - {feature}: {pct:.1f}% missing")
        logger.info("XGBoost will handle missing values, but features with too many missing values may be uninformative")
    
    # Log feature statistics
    logger.info(f"Training set shape: {X_train.shape}, class distribution: {np.bincount(y_train)}")
    logger.info(f"Validation set shape: {X_val.shape}, class distribution: {np.bincount(y_val)}")
    logger.info(f"Test set shape: {X_test.shape}, class distribution: {np.bincount(y_test)}")
    
    # Log missing value statistics
    train_missing_rates = np.zeros(X_train.shape[1])
    for i in range(X_train.shape[1]):
        # Use pandas to safely check for NaN values
        col_data = pd.Series(X_train[:, i])
        train_missing_rates[i] = (col_data.isna().sum() / len(col_data)) * 100
    
    high_missing_features = [(feature_cols[i], rate) for i, rate in enumerate(train_missing_rates) if rate > 10]
    
    if high_missing_features:
        logger.info("Features with high missing rates (>10%):")
        for feature, rate in high_missing_features[:10]:  # Show only top 10 to avoid log flood
            logger.info(f"  - {feature}: {rate:.2f}%")
        if len(high_missing_features) > 10:
            logger.info(f"  - ... and {len(high_missing_features) - 10} more")
        logger.info("XGBoost will handle these missing values optimally")
    
    if progress_tracker:
        progress_tracker.update("Feature preparation complete")
    
    return (X_train, X_val, X_test), (y_train, y_val, y_test), categorical_indices


def tune_hyperparameters(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    feature_names: List[str],
    categorical_indices: List[int] = None,
    progress_tracker: Optional[ProgressTracker] = None,
    n_trials: int = 50,
    timeout: int = 3600
) -> Dict[str, Any]:
    """
    Tune XGBoost hyperparameters using Optuna.
    
    Args:
        X_train: Training features
        y_train: Training labels
        X_val: Validation features
        y_val: Validation labels
        feature_names: List of feature names
        categorical_indices: Indices of categorical features (if any)
        progress_tracker: Optional progress tracker
        n_trials: Number of trials for optimization
        timeout: Timeout in seconds
        
    Returns:
        Dictionary of optimized hyperparameters
    """
    logger.info(f"Tuning hyperparameters with Optuna (n_trials={n_trials}, timeout={timeout}s)")
    
    # Set up device info
    device_params = detect_optimal_device()
    logger.info(f"Hyperparameter optimization using {device_params.get('tree_method', 'unknown')} method")
    
    # Define objective function for Optuna
    def objective(trial: optuna.Trial) -> float:
        """Objective function for hyperparameter optimization."""
        
        # Sample hyperparameters
        params = {
            'eta': trial.suggest_float('eta', 0.01, 0.3, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'max_leaves': trial.suggest_int('max_leaves', 31, 300),
            'min_child_weight': trial.suggest_float('min_child_weight', 1.0, 10.0, log=True),
            'gamma': trial.suggest_float('gamma', 0.0, 1.0),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'lambda': trial.suggest_float('lambda', 0.01, 10.0, log=True),
            'alpha': trial.suggest_float('alpha', 0.01, 10.0, log=True),
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.8, 1.2),
            'max_bin': trial.suggest_int('max_bin', 256, 512),
            'max_cat_to_onehot': trial.suggest_int('max_cat_to_onehot', 4, 8),
            'objective': 'binary:logistic',
            'eval_metric': ['logloss', 'auc']
        }
        
        # Set tree growth policy
        params['grow_policy'] = 'lossguide'
        
        # Add device settings from outside
        if device_params:
            params.update(device_params)
        
        # XGBoost-specific parameters for handling missing values
        params['missing'] = float('nan')
        
        # Number of boosting rounds
        num_boost_round = trial.suggest_int('num_boost_round', 500, 3000)
        
        # Enable categorical feature support if needed
        if categorical_indices and len(categorical_indices) > 0:
            params['enable_categorical'] = True
        
        try:
            # Ensure feature_names list matches the actual number of columns in X_train
            feature_names_adjusted = feature_names.copy()
            if len(feature_names_adjusted) != X_train.shape[1]:
                logger.warning(f"Feature names length ({len(feature_names_adjusted)}) doesn't match data columns ({X_train.shape[1]}). Adjusting...")
                # Option 1: Truncate feature_names if it's too long
                if len(feature_names_adjusted) > X_train.shape[1]:
                    feature_names_adjusted = feature_names_adjusted[:X_train.shape[1]]
                    logger.warning(f"Truncated feature_names to {len(feature_names_adjusted)} entries")
                # Option 2: Add generic names if feature_names is too short
                else:
                    additional_names = [f"feature_{i}" for i in range(len(feature_names_adjusted), X_train.shape[1])]
                    feature_names_adjusted = feature_names_adjusted + additional_names
                    logger.warning(f"Extended feature_names with {len(additional_names)} generic names")
                
                # Update categorical indices if needed
                categorical_indices_adjusted = [idx for idx in (categorical_indices or []) if idx < X_train.shape[1]]
            else:
                categorical_indices_adjusted = categorical_indices

            # Create DMatrix objects with feature names and enable_categorical if needed
            try:
                dtrain = xgb.DMatrix(
                    X_train, 
                    label=y_train, 
                    feature_names=feature_names_adjusted,
                    enable_categorical=params.get('enable_categorical', False)
                )
                
                dval = xgb.DMatrix(
                    X_val, 
                    label=y_val, 
                    feature_names=feature_names_adjusted,
                    enable_categorical=params.get('enable_categorical', False)
                )
            except Exception as e:
                logger.error(f"Error creating DMatrix for hyperparameter tuning: {str(e)}")
                logger.error(f"X_train shape: {X_train.shape}, feature_names_adjusted length: {len(feature_names_adjusted)}")
                raise OptimizationFailedError(f"DMatrix creation failed: {str(e)}")
            
            pruning_callback = optuna.integration.XGBoostPruningCallback(
                trial, 'validation-logloss'
            )
            
            evals_result = {}
            
            xgb.train(
                params,
                dtrain,
                num_boost_round=num_boost_round,
                evals=[(dtrain, 'train'), (dval, 'validation')],
                early_stopping_rounds=50,
                callbacks=[pruning_callback],
                evals_result=evals_result,
                verbose_eval=False
            )
            
            # Return validation logloss (to be minimized)
            val_logloss = evals_result['validation']['logloss'][-1]
            
            # Return best score
            return val_logloss
            
        except OptimizationFailedError as e:
            # Re-raise existing optimization errors
            raise
        except Exception as e:
            logger.error(f"Error in trial: {str(e)}")
            # This allows Optuna to record the trial as failed and continue
            raise optuna.exceptions.TrialPruned(f"Trial failed with error: {str(e)}")
    
    # Create Optuna study with pruning to eliminate unpromising trials early
    study_name = f"xgboost_tennis_prediction_{time.strftime('%Y%m%d_%H%M%S')}"
    study = optuna.create_study(
        direction="minimize",  # Minimize logloss
        study_name=study_name,
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
    )
    
    # Run hyperparameter optimization
    try:
        study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)
        
        # Get best parameters
        best_params = study.best_params
        
        # Extract num_boost_round from best params
        num_boost_round = best_params.pop('num_boost_round', 1000)
        
        # Get optimal device parameters again for final model
        final_device_params = detect_optimal_device()
        
        # Add fixed parameters
        best_params.update({
            'objective': 'binary:logistic',
            'eval_metric': ['logloss', 'auc'],  # Track both metrics
            'grow_policy': 'lossguide',
            'missing': float('nan'),
            'num_boost_round': num_boost_round
        })
        
        # Add device-specific parameters
        best_params.update(final_device_params)
        
        # Enable categorical support if needed
        if categorical_indices and len(categorical_indices) > 0:
            best_params['enable_categorical'] = True
        
        # Log best parameters
        logger.info(f"Best hyperparameters: {best_params}")
        logger.info(f"Best validation logloss: {study.best_value:.6f}")
        
        # Save study results
        with open(OPTUNA_STUDY_OUTPUT, 'wb') as f:
            pickle.dump(study, f)
        logger.info(f"Optuna study saved to {OPTUNA_STUDY_OUTPUT}")
        
        # Create optimization history plot
        try:
            fig = plot_optimization_history(study)
            fig.write_image(str(PLOTS_DIR / "optuna_history.png"))
            
            fig = plot_param_importances(study)
            fig.write_image(str(PLOTS_DIR / "optuna_param_importance.png"))
            
            logger.info("Optuna plots saved to plots directory")
        except Exception as e:
            logger.warning(f"Failed to create Optuna plots: {e}")
        
        if progress_tracker:
            progress_tracker.update(f"Hyperparameter tuning complete - best validation logloss: {study.best_value:.6f}")
        
        return best_params
    
    except Exception as e:
        logger.error(f"Error in hyperparameter tuning: {e}")
        logger.info("Falling back to default parameters")
        
        # Return default parameters on error
        default_params = BASELINE_PARAMS.copy()
        
        # Enable categorical support if needed
        if categorical_indices and len(categorical_indices) > 0:
            default_params['enable_categorical'] = True
            
        if progress_tracker:
            progress_tracker.update("Using default parameters due to tuning error")
            
        return default_params


def train_model(
    X_train: np.ndarray, 
    y_train: np.ndarray, 
    X_val: np.ndarray, 
    y_val: np.ndarray, 
    feature_names: list[str],
    categorical_features: list[int] = None,
    params: dict[str, Any] = None,
    early_stopping_rounds: int = 50,
    progress_tracker: Optional[ProgressTracker] = None
) -> tuple[xgb.Booster, dict[str, list[float]]]:
    """
    Train an XGBoost model with early stopping.
    
    Args:
        X_train: Training feature matrix
        y_train: Training target vector
        X_val: Validation feature matrix
        y_val: Validation target vector
        feature_names: List of feature names
        categorical_features: List of indices of categorical features
        params: XGBoost parameters (uses BASELINE_PARAMS if None)
        early_stopping_rounds: Number of rounds for early stopping
        progress_tracker: Optional progress tracker
        
    Returns:
        Tuple of (trained model, evaluation results)
    """
    logger.info("Training XGBoost model with early stopping")
    
    start_time = time.time()
    
    # Check if we have data and features
    if X_train.shape[0] == 0 or X_train.shape[1] == 0:
        raise ValueError(f"Training data has invalid shape: {X_train.shape}")
    
    if X_val.shape[0] == 0 or X_val.shape[1] == 0:
        raise ValueError(f"Validation data has invalid shape: {X_val.shape}")
    
    if not feature_names:
        logger.warning("No feature names provided. Using generic feature names.")
        feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]
    
    if params is None:
        # Start with baseline params
        params = BASELINE_PARAMS.copy()
    else:
        # If params are provided but don't have device settings, add them
        if 'tree_method' not in params:
            device_params = detect_optimal_device()
            params.update(device_params)
    
    # Enable categorical feature support if categorical features are present
    if categorical_features and len(categorical_features) > 0:
        params['enable_categorical'] = True
        logger.info(f"Enabling categorical feature support for {len(categorical_features)} features")
    
    # Create DMatrix format for better memory efficiency and faster processing
    logger.info("Creating DMatrix for efficient memory usage")
    
    # Ensure feature_names list matches the actual number of columns in X_train
    if len(feature_names) != X_train.shape[1]:
        logger.warning(f"Feature names length ({len(feature_names)}) doesn't match data columns ({X_train.shape[1]}). Adjusting...")
        # Option 1: Truncate feature_names if it's too long
        if len(feature_names) > X_train.shape[1]:
            feature_names = feature_names[:X_train.shape[1]]
            logger.warning(f"Truncated feature_names to {len(feature_names)} entries")
        # Option 2: Add generic names if feature_names is too short
        else:
            additional_names = [f"feature_{i}" for i in range(len(feature_names), X_train.shape[1])]
            feature_names = feature_names + additional_names
            logger.warning(f"Extended feature_names with {len(additional_names)} generic names")
        
        # Update categorical features to match the new feature list if needed
        if categorical_features:
            categorical_features = [idx for idx in categorical_features if idx < X_train.shape[1]]
    
    # Create DMatrix objects with feature names
    try:
        dtrain = xgb.DMatrix(
            X_train, 
            label=y_train, 
            feature_names=feature_names,
            enable_categorical=params.get('enable_categorical', False)
        )
        
        dval = xgb.DMatrix(
            X_val, 
            label=y_val, 
            feature_names=feature_names,
            enable_categorical=params.get('enable_categorical', False)
        )
    except Exception as e:
        logger.error(f"Error creating DMatrix: {str(e)}")
        logger.error(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
        logger.error(f"Feature names: {feature_names[:10]}{'...' if len(feature_names) > 10 else ''}")
        raise
    
    # Set up evaluation metrics
    watchlist = [(dtrain, 'train'), (dval, 'validation')]
    
    # Create callback for progress tracking
    callbacks = []
    if progress_tracker:
        progress_callback = XGBoostProgressCallback(params.get('num_boost_round', 1000))
        callbacks.append(progress_callback)
    
    # Train model with early stopping
    logger.info(f"Starting training with early stopping (patience: {early_stopping_rounds})")
    
    num_boost_round = params.pop('num_boost_round', 1000)
    
    # Log memory usage before training
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    logger.info(f"Memory usage before training: {memory_before:.2f} MB")
    
    # Initialize dictionary to store evaluation results
    evals_result_dict = {}
    
    # Train the model
    try:
        model = xgb.train(
            params,
            dtrain,
            num_boost_round=num_boost_round,
            evals=watchlist,
            early_stopping_rounds=early_stopping_rounds,
            callbacks=callbacks,
            verbose_eval=100,  # Log progress every 100 rounds
            evals_result=evals_result_dict  # Store evaluation results here
        )
        
        # Log memory usage after training
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        logger.info(f"Memory usage after training: {memory_after:.2f} MB (change: {memory_after - memory_before:.2f} MB)")
        
        # Log best iteration and score
        logger.info(f"Best iteration: {model.best_iteration}")
        logger.info(f"Best validation score: {model.best_score}")
        logger.info(f"Training completed in {time.time() - start_time:.2f} seconds")
    except Exception as e:
        logger.error(f"Error during training: {str(e)}")
        raise
    
    if progress_tracker:
        progress_tracker.update(f"Model training completed (best iteration: {model.best_iteration})")
    
    return model, evals_result_dict  # Return model and populated evals_result


def evaluate_model(
    model: xgb.Booster, 
    X: np.ndarray, 
    y: np.ndarray, 
    feature_names: List[str],
    dataset_name: str = "Test",
    threshold: float = 0.5,
    surfaces: List[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> Dict[str, Any]:
    """
    Evaluate model performance on a dataset with comprehensive metrics.
    
    Args:
        model: Trained XGBoost model
        X: Feature matrix
        y: Target vector
        feature_names: List of feature names
        dataset_name: Name of the dataset for reporting
        threshold: Classification threshold
        surfaces: List of surfaces to evaluate separately
        progress_tracker: Optional progress tracker
        
    Returns:
        Dictionary of evaluation metrics
    """
    if progress_tracker:
        progress_tracker.update(f"Evaluating model on {dataset_name} dataset")
    
    logger.info(f"Evaluating model on {dataset_name} dataset with {X.shape[0]} samples")
    
    # Validate input data
    if X.shape[0] == 0 or X.shape[1] == 0:
        logger.error(f"Cannot evaluate model: Input data has invalid shape {X.shape}")
        return {"error": f"Invalid data shape: {X.shape}"}
    
    if len(y) == 0:
        logger.error("Cannot evaluate model: Empty target vector")
        return {"error": "Empty target vector"}
    
    if not feature_names:
        logger.warning("No feature names provided. Using generic feature names.")
        feature_names = [f"feature_{i}" for i in range(X.shape[1])]
    
    # Create DMatrix for efficient prediction
    device_params = detect_optimal_device()
    
    # Ensure feature names match the data dimensions
    if len(feature_names) != X.shape[1]:
        logger.warning(f"Feature names length ({len(feature_names)}) doesn't match data columns ({X.shape[1]}) in evaluation. Adjusting...")
        # Truncate or extend feature names as needed
        if len(feature_names) > X.shape[1]:
            feature_names = feature_names[:X.shape[1]]
        else:
            additional_names = [f"feature_{i}" for i in range(len(feature_names), X.shape[1])]
            feature_names = feature_names + additional_names
    
    try:
        dtest = xgb.DMatrix(X, label=y, feature_names=feature_names)
        
        # Make predictions
        y_pred_proba = model.predict(dtest)
        y_pred = (y_pred_proba > threshold).astype(int)
    except Exception as e:
        logger.error(f"Error during prediction: {str(e)}")
        return {"error": f"Prediction error: {str(e)}"}
    
    # Initialize results dictionary
    metrics = {
        "overall": {},
        "by_surface": {} if surfaces else None
    }
    
    try:
        # Calculate standard classification metrics
        accuracy = accuracy_score(y, y_pred)
        balanced_acc = balanced_accuracy_score(y, y_pred)  # Add balanced accuracy
        precision = precision_score(y, y_pred)
        recall = recall_score(y, y_pred)
        f1 = f1_score(y, y_pred)
        
        # Calculate ROC AUC
        try:
            roc_auc = roc_auc_score(y, y_pred_proba)  # Use roc_auc_score directly
        except Exception as e:
            logger.warning(f"Could not calculate ROC AUC: {e}")
            roc_auc = None
        
        # Calculate average precision (PR AUC)
        try:
            pr_auc = average_precision_score(y, y_pred_proba)
        except Exception as e:
            logger.warning(f"Could not calculate PR AUC: {e}")
            pr_auc = None
        
        # Calculate log loss
        try:
            logloss = log_loss(y, y_pred_proba)
        except Exception as e:
            logger.warning(f"Could not calculate log loss: {e}")
            logloss = None
        
        # Calculate Brier score (calibration metric)
        try:
            brier = brier_score_loss(y, y_pred_proba)
        except Exception as e:
            logger.warning(f"Could not calculate Brier score: {e}")
            brier = None
        
        # Confusion matrix for showing class distribution
        try:
            cm = confusion_matrix(y, y_pred)
            tn, fp, fn, tp = cm.ravel()
        except Exception:
            tn, fp, fn, tp = 0, 0, 0, 0
        
        # Get detailed classification report
        try:
            class_report = classification_report(y, y_pred, output_dict=True)
        except Exception as e:
            logger.warning(f"Could not generate classification report: {e}")
            class_report = {}
    
        # Store overall metrics
        metrics["overall"] = {
            "accuracy": accuracy,
            "balanced_accuracy": balanced_acc,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "roc_auc": roc_auc,
            "pr_auc": pr_auc,
            "log_loss": logloss,
            "brier_score": brier,
            "tn": int(tn),
            "fp": int(fp),
            "fn": int(fn),
            "tp": int(tp),
            "confusion_matrix": cm.tolist() if 'cm' in locals() else None,
            "classification_report": class_report
        }
    
        # Evaluate by surface if requested
        if surfaces:
            surface_col_idx = None
            
            # Look for surface column in feature names
            for i, name in enumerate(feature_names):
                if name.lower() == 'surface':
                    surface_col_idx = i
                    break
            
            if surface_col_idx is not None:
                logger.info("Evaluating performance by surface...")
                surface_values = X[:, surface_col_idx]
                
                for surface_idx, surface in enumerate(surfaces):
                    # Find rows with this surface
                    surface_mask = surface_values == surface_idx
                    if np.sum(surface_mask) > 0:
                        surface_y = y[surface_mask]
                        surface_pred = y_pred[surface_mask]
                        surface_pred_proba = y_pred_proba[surface_mask]
                        
                        # Calculate metrics for this surface
                        surface_metrics = {
                            "count": int(np.sum(surface_mask)),
                            "accuracy": accuracy_score(surface_y, surface_pred),
                            "balanced_accuracy": balanced_accuracy_score(surface_y, surface_pred),
                            "precision": precision_score(surface_y, surface_pred, zero_division=0),
                            "recall": recall_score(surface_y, surface_pred, zero_division=0),
                            "f1": f1_score(surface_y, surface_pred, zero_division=0)
                        }
                        
                        # Add ROC AUC and PR AUC if possible
                        if len(np.unique(surface_y)) > 1:
                            try:
                                surface_metrics["roc_auc"] = roc_auc_score(surface_y, surface_pred_proba)
                                surface_metrics["pr_auc"] = average_precision_score(surface_y, surface_pred_proba)
                            except Exception as e:
                                logger.warning(f"Could not calculate AUC metrics for surface {surface}: {e}")
                        
                        metrics["by_surface"][surface] = surface_metrics
                        
                        # Log surface metrics
                        logger.info(f"Surface '{surface}' metrics:")
                        logger.info(f"  Count: {surface_metrics['count']}")
                        logger.info(f"  Accuracy: {surface_metrics['accuracy']:.4f}")
                        if "roc_auc" in surface_metrics:
                            logger.info(f"  ROC AUC: {surface_metrics['roc_auc']:.4f}")
            else:
                logger.warning("Surface column not found in feature names. Skipping surface-specific evaluation.")
    except Exception as e:
        logger.error(f"Error calculating evaluation metrics: {str(e)}")
        metrics["error"] = str(e)
        
    return metrics


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, 
                          output_path: Optional[Union[str, Path]] = None,
                          title: str = "Confusion Matrix",
                          progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save confusion matrix.
    
    Args:
        y_true: True labels
        y_pred: Predicted labels
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    cm = confusion_matrix(y_true, y_pred)
    
    # Calculate percentages
    cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Plot counts
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax1)
    ax1.set_xlabel('Predicted')
    ax1.set_ylabel('Actual')
    ax1.set_title(f'{title} (Counts)')
    ax1.set_xticklabels(['Loss', 'Win'])
    ax1.set_yticklabels(['Loss', 'Win'])
    
    # Plot percentages
    sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', cbar=False, ax=ax2)
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')
    ax2.set_title(f'{title} (Percentages)')
    ax2.set_xticklabels(['Loss', 'Win'])
    ax2.set_yticklabels(['Loss', 'Win'])
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved confusion matrix to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def plot_feature_importance(importances: Dict[str, float], top_n: int = 20, 
                           output_path: Optional[Union[str, Path]] = None,
                           progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save feature importance.
    
    Args:
        importances: Dictionary of feature importances
        top_n: Number of top features to plot
        output_path: Path to save the plot
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting top {top_n} feature importances")
    
    # Sort and limit to top N
    sorted_importances = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:top_n])
    
    # Calculate total importance for normalization
    total_importance = sum(sorted_importances.values())
    normalized_importances = {k: v/total_importance*100 for k, v in sorted_importances.items()}
    
    # Reverse order for horizontal bar chart (to have highest at top)
    features = list(reversed(list(normalized_importances.keys())))
    values = list(reversed(list(normalized_importances.values())))
    
    # Plot
    plt.figure(figsize=(12, 10))
    
    # Plot horizontal bar chart with percentages
    bars = plt.barh(features, values, color='cornflowerblue')
    
    # Add percentage labels
    for i, bar in enumerate(bars):
        plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, f'{values[i]:.1f}%',
                va='center', fontsize=10)
    
    plt.xlabel('Relative Importance (%)')
    plt.title(f'Top {top_n} Feature Importances')
    plt.gca().spines[['right', 'top']].set_visible(False)  # Clean up
    plt.grid(axis='x', linestyle='--', alpha=0.6)
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved feature importance plot to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update("Feature importance plotting complete")


def plot_roc_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                  output_path: Optional[Union[str, Path]] = None,
                  title: str = "ROC Curve",
                  progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save ROC curve.
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    
    # Plot thresholds
    threshold_markers = [0.1, 0.3, 0.5, 0.7, 0.9]
    for threshold in threshold_markers:
        # Find closest threshold
        idx = (np.abs(thresholds - threshold)).argmin()
        plt.plot(fpr[idx], tpr[idx], 'o', markersize=8, 
                label=f'Threshold = {thresholds[idx]:.2f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.grid(alpha=0.3)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved ROC curve to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def plot_precision_recall_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                              output_path: Optional[Union[str, Path]] = None,
                              title: str = "Precision-Recall Curve",
                              progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save precision-recall curve.
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    # Calculate PR curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
    pr_auc = average_precision_score(y_true, y_pred_proba)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.plot(recall, precision, color='green', lw=2, label=f'PR curve (AP = {pr_auc:.3f})')
    
    # Add baseline
    baseline = sum(y_true) / len(y_true)
    plt.axhline(y=baseline, color='navy', lw=2, linestyle='--', label=f'Baseline (= {baseline:.3f})')
    
    # Plot thresholds
    threshold_markers = [0.1, 0.3, 0.5, 0.7, 0.9]
    for threshold in threshold_markers:
        if len(thresholds) > 0:  # Check if we have thresholds
            # Find closest threshold (handling edge case)
            idx = min(len(thresholds) - 1, (np.abs(thresholds - threshold)).argmin())
            idx2 = min(len(precision) - 1, idx)  # Ensure we don't exceed precision length
            plt.plot(recall[idx2], precision[idx2], 'o', markersize=8, 
                    label=f'Threshold = {thresholds[idx]:.2f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title)
    plt.legend(loc='lower left')
    plt.grid(alpha=0.3)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved precision-recall curve to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def plot_calibration_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                          output_path: Optional[Union[str, Path]] = None,
                          title: str = "Calibration Curve",
                          progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Plot and save calibration curve.
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        output_path: Path to save the plot
        title: Plot title
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Plotting {title}")
    
    # Calculate calibration curve
    prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=10)
    
    # Calculate Brier score
    brier = brier_score_loss(y_true, y_pred_proba)
    
    # Plot
    plt.figure(figsize=(10, 8))
    
    # Plot calibration curve
    plt.plot(prob_pred, prob_true, 's-', color='darkgreen', lw=2, 
            label=f'Calibration curve (Brier score = {brier:.3f})')
    
    # Plot perfect calibration
    plt.plot([0, 1], [0, 1], '--', color='gray', label='Perfect calibration')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('Mean predicted probability')
    plt.ylabel('Fraction of positives (Empirical probability)')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.grid(alpha=0.3)
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved calibration curve to {output_path}")
    
    plt.close()
    
    if progress_tracker:
        progress_tracker.update(f"{title} plotting complete")


def save_metrics(metrics: Dict[str, Any], output_path: Union[str, Path],
                progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Save evaluation metrics to JSON.
    
    Args:
        metrics: Dictionary of metrics
        output_path: Path to save metrics
        progress_tracker: Optional progress tracker
    """
    logger.info(f"Saving metrics to {output_path}")
    
    # Convert numpy types to Python types for JSON serialization
    def convert_numpy_types(obj):
        if isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_numpy_types(x) for x in obj]
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return convert_numpy_types(obj.tolist())
        elif isinstance(obj, datetime):
            return obj.isoformat()
        elif obj is None:
            return None
        else:
            return obj
    
    metrics_json = convert_numpy_types(metrics)
    
    # Add timestamp
    metrics_json['timestamp'] = datetime.now().isoformat()
    
    # Add model information
    metrics_json['model_info'] = {
        'model_path': str(MODEL_OUTPUT),
        'feature_count': metrics_json.get('feature_count', None),
        'data_size': metrics_json.get('data_size', None)
    }
    
    # Save to file
    with open(output_path, 'w') as f:
        json.dump(metrics_json, f, indent=2)
    
    logger.info(f"Metrics saved to {output_path}")
    
    if progress_tracker:
        progress_tracker.update("Metrics saving complete")


def evaluate_player_position_bias(model: xgb.Booster, test_df: pd.DataFrame, feature_cols: List[str],
                                 progress_tracker: Optional[ProgressTracker] = None) -> Dict[str, float]:
    """
    Evaluate if the model has any bias based on player position.
    
    Args:
        model: Trained XGBoost model
        test_df: Test DataFrame
        feature_cols: List of feature column names
        progress_tracker: Optional progress tracker
        
    Returns:
        Dictionary with bias metrics
    """
    logger.info("Evaluating player position bias")
    
    # Get unique match IDs
    match_ids = test_df['match_id'].unique()
    
    # Initialize counters
    same_prediction_count = 0
    different_prediction_count = 0
    
    # Create a progress bar
    pbar = tqdm(match_ids, desc="Checking player position bias")
    
    for match_id in pbar:
        # Get the two rows for this match (player1 as winner and player1 as loser)
        match_rows = test_df[test_df['match_id'] == match_id]
        
        if len(match_rows) != 2:
            continue
        
        # Get features for both scenarios
        X1 = match_rows.iloc[0][feature_cols].values.reshape(1, -1)
        X2 = match_rows.iloc[1][feature_cols].values.reshape(1, -1)
        
        # Create DMatrix objects
        dmat1 = xgb.DMatrix(X1, feature_names=feature_cols)
        dmat2 = xgb.DMatrix(X2, feature_names=feature_cols)
        
        # Get predictions
        pred1 = model.predict(dmat1)[0]
        pred2 = model.predict(dmat2)[0]
        
        # Check if predictions are consistent
        # pred1 > 0.5 should mean pred2 < 0.5 (and vice versa)
        # If both are on the same side of 0.5, we have inconsistency
        if (pred1 > 0.5 and pred2 > 0.5) or (pred1 < 0.5 and pred2 < 0.5):
            different_prediction_count += 1
        else:
            same_prediction_count += 1
    
    # Calculate consistency percentage
    total_matches = same_prediction_count + different_prediction_count
    if total_matches > 0:
        consistency_pct = (same_prediction_count / total_matches) * 100
    else:
        consistency_pct = 0
    
    logger.info(f"Player position consistency: {consistency_pct:.2f}%")
    logger.info(f"Consistent predictions: {same_prediction_count} / {total_matches}")
    logger.info(f"Inconsistent predictions: {different_prediction_count} / {total_matches}")
    
    bias_metrics = {
        'player_position_consistency_pct': consistency_pct,
        'consistent_predictions': same_prediction_count,
        'inconsistent_predictions': different_prediction_count,
        'total_matches': total_matches
    }
    
    if progress_tracker:
        progress_tracker.update("Player position bias analysis complete")
    
    return bias_metrics


def select_features_by_importance(
    importances: dict[str, float], 
    threshold: float = 0.95, 
    min_features: int = 20
) -> list[str]:
    """
    Select features based on cumulative importance up to a threshold.
    
    Args:
        importances: Dictionary of feature importance scores
        threshold: Cumulative importance threshold (0.0-1.0)
        min_features: Minimum number of features to select
        
    Returns:
        List of selected feature names
    """
    # Check if importances dictionary is empty
    if not importances:
        logger.warning("Feature importance dictionary is empty. Cannot select features.")
        return []
        
    # Sort features by importance
    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)
    
    # Calculate total importance
    total_importance = sum(importances.values())
    
    # If total importance is zero, return all features
    if total_importance == 0:
        logger.warning("Total feature importance is zero. Returning all features.")
        return list(importances.keys())
    
    # Select features up to threshold
    selected_features = []
    cumulative_importance = 0.0
    
    for feature, importance in sorted_features:
        selected_features.append(feature)
        cumulative_importance += importance / total_importance
        
        if cumulative_importance >= threshold and len(selected_features) >= min_features:
            break
    
    # Ensure minimum number of features
    if len(selected_features) < min_features:
        remaining = [f for f, _ in sorted_features if f not in selected_features]
        selected_features.extend(remaining[:min_features - len(selected_features)])
    
    # Safety check: If still no features selected, return all features
    if not selected_features and importances:
        logger.warning("No features selected. Returning all features.")
        selected_features = list(importances.keys())
    
    logger.info(f"Selected {len(selected_features)} features covering {cumulative_importance:.2%} of total importance")
    return selected_features


def save_metrics(metrics: dict, output_file: Path, progress_tracker: Optional[ProgressTracker] = None) -> None:
    """
    Save evaluation metrics to a JSON file.
    
    Args:
        metrics: Dictionary of metrics
        output_file: Path to save metrics
        progress_tracker: Optional progress tracker
    """
    # Convert numpy types to Python native types for JSON serialization
    def convert_to_serializable(obj):
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(i) for i in obj]
        else:
            return obj
    
    serializable_metrics = convert_to_serializable(metrics)
    
    with open(output_file, 'w') as f:
        json.dump(serializable_metrics, f, indent=4)
    
    if progress_tracker:
        progress_tracker.update("Metrics saved")


def plot_feature_importance(
    importances: dict[str, float], 
    top_n: int, 
    output_file: Path,
    progress_tracker: Optional[ProgressTracker] = None
) -> None:
    """
    Plot feature importance and save to file.
    
    Args:
        importances: Dictionary of feature importance scores
        top_n: Number of top features to display
        output_file: Path to save the plot
        progress_tracker: Optional progress tracker
    """
    plt.figure(figsize=(12, 8))
    
    # Get top features
    top_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:top_n]
    features, scores = zip(*top_features)
    
    # Create horizontal bar chart
    y_pos = np.arange(len(features))
    plt.barh(y_pos, scores, align='center')
    plt.yticks(y_pos, [f[:30] + '...' if len(f) > 30 else f for f in features])
    plt.xlabel('Importance Score')
    plt.title(f'Top {top_n} Feature Importance')
    plt.tight_layout()
    
    # Save the plot
    plt.savefig(output_file, dpi=300)
    plt.close()
    
    if progress_tracker:
        progress_tracker.update("Feature importance plot created")


def plot_roc_curve(y_true: np.ndarray, y_pred_proba: np.ndarray, output_file: Path) -> None:
    """
    Plot ROC curve and save to file.
    
    Args:
        y_true: True binary labels
        y_pred_proba: Predicted probabilities
        output_file: Path to save the plot
    """
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_precision_recall_curve(y_true: np.ndarray, y_pred_proba: np.ndarray, output_file: Path) -> None:
    """
    Plot Precision-Recall curve and save to file.
    
    Args:
        y_true: True binary labels
        y_pred_proba: Predicted probabilities
        output_file: Path to save the plot
    """
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    pr_auc = average_precision_score(y_true, y_pred_proba)
    
    plt.figure(figsize=(10, 8))
    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.3f})')
    plt.axhline(y=sum(y_true)/len(y_true), color='red', linestyle='--', label=f'Baseline (positive rate = {sum(y_true)/len(y_true):.3f})')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.grid(True, alpha=0.3)
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_calibration_curve(y_true: np.ndarray, y_pred_proba: np.ndarray, output_file: Path) -> None:
    """
    Plot calibration curve and save to file.
    
    Args:
        y_true: True binary labels
        y_pred_proba: Predicted probabilities
        output_file: Path to save the plot
    """
    plt.figure(figsize=(10, 8))
    
    # Calculate calibration curve
    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred_proba, n_bins=10)
    
    # Plot perfect calibration
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    
    # Plot model calibration
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Model")
    
    # Calculate and display Brier score
    brier = brier_score_loss(y_true, y_pred_proba)
    plt.title(f'Calibration Curve (Brier Score: {brier:.3f})')
    
    plt.xlabel('Mean predicted probability')
    plt.ylabel('Fraction of positives')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, output_file: Path, title: str) -> None:
    """
    Plot confusion matrix and save to file.
    
    Args:
        y_true: True binary labels
        y_pred: Predicted binary labels
        output_file: Path to save the plot
        title: Title for the plot
    """
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 8))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    
    tick_marks = np.arange(2)
    plt.xticks(tick_marks, ['Loser', 'Winner'])
    plt.yticks(tick_marks, ['Loser', 'Winner'])
    
    # Add text annotations
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
    
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_shap_values(model: xgb.Booster, X: np.ndarray, feature_names: list[str], output_file: Path) -> None:
    """
    Plot SHAP values for feature explanation and save to file.
    
    Args:
        model: Trained XGBoost model
        X: Feature matrix
        feature_names: List of feature names
        output_file: Path to save the plot
    """
    try:
        import shap
    except ImportError:
        logger.warning("SHAP package not installed. Skipping SHAP plot.")
        return
    
    # Create DMatrix
    dmatrix = xgb.DMatrix(X, feature_names=feature_names)
    
    # Get SHAP values
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(dmatrix)
    
    # Create summary plot
    plt.figure(figsize=(10, 12))
    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
    plt.tight_layout()
    
    # Save the plot
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()


def save_pipeline(
    model: xgb.Booster, 
    features: list[str], 
    feature_transformer: Optional[Any],
    output_file: Path,
    progress_tracker: Optional[ProgressTracker] = None
) -> None:
    """
    Save the model pipeline for easy prediction.
    
    Args:
        model: Trained XGBoost model
        features: List of feature names
        feature_transformer: Any preprocessing transformer (optional)
        output_file: Path to save the pipeline
        progress_tracker: Optional progress tracker
    """
    pipeline = {
        'model': model,
        'features': features,
        'feature_transformer': feature_transformer,
        'metadata': {
            'creation_date': datetime.now().isoformat(),
            'model_type': 'xgboost',
            'feature_count': len(features),
            'version': '3.0'
        }
    }
    
    with open(output_file, 'wb') as f:
        pickle.dump(pipeline, f)
    
    if progress_tracker:
        progress_tracker.update("Pipeline saved")


def main() -> None:
    """
    Main function for training tennis prediction model.
    """
    start_time = time.time()
    
    # Create output directories if they don't exist
    os.makedirs(MODELS_DIR, exist_ok=True)
    os.makedirs(PLOTS_DIR, exist_ok=True)
    
    # Log hardware information and model configuration
    logger.info(f"XGBoost version: {xgb.__version__}")
    logger.info(f"Python version: {sys.version}")
    logger.info(f"CPU count: {os.cpu_count()}")
    logger.info(f"Memory: {psutil.virtual_memory().total / (1024 ** 3):.1f} GB")
    
    # Check CUDA availability for GPU acceleration
    device_info = detect_optimal_device()
    if device_info.get('tree_method') == 'gpu_hist':
        logger.info("Training will use GPU acceleration (CUDA)")
    else:
        logger.info(f"Training will use CPU with {device_info.get('nthread', 'default')} threads")
    
    # Define the total number of processing steps for progress tracking
    total_steps = 10
    progress_tracker = ProgressTracker(total_steps, "Tennis Model Training")
    
    try:
        # Step 1: Load data from PostgreSQL database
        logger.info(f"Step 1/{total_steps}: Loading data from database...")
        df = load_data_from_database(progress_tracker=progress_tracker)
        logger.info(f"Loaded {len(df)} samples spanning from "
                   f"{df['tournament_date'].min().date()} to {df['tournament_date'].max().date()}")
        
        # Check if we have enough data
        if len(df) < 1000:
            logger.warning(f"Potentially insufficient data: only {len(df)} samples")
        
        # Step 2: Create time-based train/val/test split
        logger.info(f"Step 2/{total_steps}: Creating strict time-based split...")
        train_df, val_df, test_df = create_time_based_train_val_test_split(
            df, TRAIN_SPLIT, VAL_SPLIT, TEST_SPLIT, progress_tracker
        )
        
        # Step 3: Get feature columns
        logger.info(f"Step 3/{total_steps}: Identifying feature columns...")
        feature_cols = get_feature_columns(df, progress_tracker)
        
        # Step 4: Prepare features and labels - no scaling for XGBoost
        logger.info(f"Step 4/{total_steps}: Preparing features and labels...")
        (X_train, X_val, X_test), (y_train, y_val, y_test), categorical_indices = prepare_features(
            train_df, val_df, test_df, feature_cols, progress_tracker
        )
        
        # Step 5: Tune hyperparameters
        logger.info(f"Step 5/{total_steps}: Tuning hyperparameters...")
        best_params = tune_hyperparameters(
            X_train, y_train, X_val, y_val, feature_cols, categorical_indices, progress_tracker
        )
        
        # Step 6: Train model with best parameters
        logger.info(f"Step 6/{total_steps}: Training model with tuned hyperparameters...")
        model, evals_result = train_model(
            X_train, y_train, X_val, y_val, feature_cols, categorical_indices, best_params, 50, progress_tracker
        )
        
        # Step 7: Extract feature importance
        logger.info(f"Step 7/{total_steps}: Extracting feature importance...")
        try:
            # Get feature importance scores
            importance_scores = model.get_score(importance_type='gain')
            
            # Convert to dictionary and ensure feature names match
            importances = {}
            for feature, score in importance_scores.items():
                if feature in feature_cols:
                    importances[feature] = float(score)
            
            # Sort by importance
            importances = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True))
            
            # Log top features
            if importances:
                top_features = list(importances.items())[:10]
                logger.info("Top 10 features by importance:")
                for feature, importance in top_features:
                    logger.info(f"  - {feature}: {importance:.4f}")
            else:
                logger.warning("No feature importance scores were extracted")
                
        except Exception as e:
            logger.error(f"Error extracting feature importance: {str(e)}")
            importances = {}
            logger.warning("Using empty importance dictionary. Feature selection will return all features.")
        
        # Step 8: Select most important features
        logger.info(f"Step 8/{total_steps}: Selecting top features...")
        
        # If no feature importance was extracted, use all features
        if not importances:
            logger.warning("No feature importance was extracted. Using all features.")
            selected_features = feature_cols
        else:
            selected_features = select_features_by_importance(importances, threshold=0.95, min_features=20)
            if not selected_features:
                logger.warning("No features were selected. Using all feature columns.")
                selected_features = feature_cols
        
        # Step 9: Retrain with selected features
        logger.info(f"Step 9/{total_steps}: Retraining with selected features...")
        
        # Create feature matrices with selected features
        try:
            # Get indices of selected features
            feature_indices = []
            for f in selected_features:
                if f in feature_cols:
                    idx = feature_cols.index(f)
                    if idx < X_train.shape[1]:  # Ensure index is valid
                        feature_indices.append(idx)
            
            # Verify we have valid indices
            if not feature_indices:
                logger.error("No valid feature indices found. Cannot continue.")
                raise ValueError("No valid feature indices for retraining")
            
            # Create selected feature matrices
            X_train_selected = X_train[:, feature_indices]
            X_val_selected = X_val[:, feature_indices]
            X_test_selected = X_test[:, feature_indices]
            
            # Verify shapes
            logger.info(f"Selected feature shapes - Train: {X_train_selected.shape}, Val: {X_val_selected.shape}, Test: {X_test_selected.shape}")
            
            # Update categorical indices for selected features
            selected_categorical_indices = []
            for i, feature in enumerate(selected_features):
                if feature in feature_cols and feature_cols.index(feature) in categorical_indices:
                    selected_categorical_indices.append(i)
            
            # Retrain with selected features
            final_model, final_evals_result = train_model(
                X_train_selected, y_train, X_val_selected, y_val, selected_features, 
                selected_categorical_indices, best_params, 50, progress_tracker
            )
            
        except Exception as e:
            logger.error(f"Error in retraining with selected features: {str(e)}")
            logger.warning("Falling back to original model")
            final_model = model
            final_evals_result = evals_result
            selected_features = feature_cols
            X_train_selected = X_train
            X_val_selected = X_val
            X_test_selected = X_test
        
        # Step 10: Evaluate model
        logger.info(f"Step 10/{total_steps}: Evaluating model...")
        test_metrics = evaluate_model(
            final_model, X_test_selected, y_test, selected_features, 
            dataset_name="Test",
            threshold=0.5,
            surfaces=SURFACES,
            progress_tracker=progress_tracker
        )
        
        # Also evaluate on validation set to compare
        val_metrics = evaluate_model(
            final_model, X_val_selected, y_val, selected_features, 
            dataset_name="Validation",
            threshold=0.5,
            surfaces=None  # Skip surface-specific metrics for validation
        )
        
        # Check for potential overfitting
        val_acc = val_metrics['overall']['accuracy']
        test_acc = test_metrics['overall']['accuracy']
        acc_diff = abs(val_acc - test_acc)
        
        if acc_diff > 0.05:
            logger.warning(f"Potential overfitting: validation accuracy {val_acc:.4f} vs test accuracy {test_acc:.4f}")
            logger.warning(f"Difference: {acc_diff:.4f} - model may not generalize well to new data")
        else:
            logger.info(f"Model generalizes well: validation accuracy {val_acc:.4f} vs test accuracy {test_acc:.4f}")
        
        # Step 11: Save metrics
        logger.info(f"Step 11/{total_steps}: Saving evaluation metrics...")
        combined_metrics = {
            'test': test_metrics,
            'validation': val_metrics,
            'selected_features': selected_features,
            'feature_importance': {k: float(v) for k, v in importances.items()},
            'training_info': {
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'test_samples': len(X_test),
                'training_date': datetime.now().isoformat(),
                'data_date_range': {
                    'start': df['tournament_date'].min().isoformat(),
                    'end': df['tournament_date'].max().isoformat()
                }
            }
        }
        save_metrics(combined_metrics, METRICS_OUTPUT, progress_tracker)
        
        # Step 12: Save hyperparameters
        logger.info(f"Step 12/{total_steps}: Saving hyperparameters...")
        with open(HYPERPARAMS_OUTPUT, "w") as f:
            json.dump(best_params, f, indent=4)
        progress_tracker.update("Hyperparameters saved")
        
        # Step 13: Plot feature importance
        logger.info(f"Step 13/{total_steps}: Plotting feature importance...")
        plot_feature_importance(
            importances, min(20, len(selected_features)), 
            PLOTS_DIR / "feature_importance.png", progress_tracker
        )
        
        # Also plot ROC and PR curves
        y_pred_proba = final_model.predict(xgb.DMatrix(X_test_selected, feature_names=selected_features))
        plot_roc_curve(y_test, y_pred_proba, PLOTS_DIR / "roc_curve.png")
        plot_precision_recall_curve(y_test, y_pred_proba, PLOTS_DIR / "pr_curve.png")
        plot_calibration_curve(y_test, y_pred_proba, PLOTS_DIR / "calibration_curve.png")
        
        # Step 14: Save model and pipeline
        logger.info(f"Step 14/{total_steps}: Saving model and pipeline...")
        final_model.save_model(str(MODEL_OUTPUT))
        logger.info(f"Model saved to {MODEL_OUTPUT}")
        
        # Save full pipeline for easy prediction
        save_pipeline(final_model, selected_features, None, PIPELINE_OUTPUT, progress_tracker)
        
        # Generate confusion matrix plot
        y_pred = (y_pred_proba >= 0.5).astype(int)
        plot_confusion_matrix(y_test, y_pred, PLOTS_DIR / "confusion_matrix_test.png", "Test Set Confusion Matrix")
        
        # Print final message
        total_time = time.time() - start_time
        logger.info(f"Model training completed in {total_time:.2f} seconds")
        logger.info(f"Model accuracy on test set: {test_metrics['overall']['accuracy']:.4f}")
        logger.info(f"Model saved to: {MODEL_OUTPUT}")
        logger.info(f"Pipeline saved to: {PIPELINE_OUTPUT}")
        logger.info(f"Metrics saved to: {METRICS_OUTPUT}")
        
        # Plot SHAP values for feature explanation if data size is manageable
        if len(X_test_selected) <= 1000:  # Limit for SHAP analysis to avoid memory issues
            sample_size = min(500, len(X_test_selected))  # Further limit sample size
            sample_indices = np.random.choice(len(X_test_selected), sample_size, replace=False)
            logger.info(f"Generating SHAP plots with {sample_size} samples...")
            plot_shap_values(
                final_model, 
                X_test_selected[sample_indices], 
                selected_features, 
                PLOTS_DIR / "shap_summary.png"
            )
        else:
            logger.info("Skipping SHAP plot generation due to large test set size")
        
    except Exception as e:
        logger.error(f"Error in training process: {str(e)}")
        logger.exception("Exception details:")
        raise


if __name__ == "__main__":
    main() 